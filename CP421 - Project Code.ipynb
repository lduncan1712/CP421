{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23214,
     "status": "ok",
     "timestamp": 1742781799195,
     "user": {
      "displayName": "Dafna Matsegora",
      "userId": "08627055149929474058"
     },
     "user_tz": 240
    },
    "id": "dHpnfhRz4hZQ",
    "outputId": "2db63b57-707a-491f-9774-7148dee6b4bf"
   },
   "outputs": [],
   "source": [
    "#!pip install torch \n",
    "\n",
    "#!pip install --upgrade --force-reinstall nltk\n",
    "\n",
    "#!pip install transformers==4.28.1\n",
    "#!pip install sentence-transformers==2.2.2\n",
    "#!pip install huggingface_hub==0.13.4\n",
    "#!pip install deepspeed\n",
    "#!pip install sentencepiece\n",
    "#!pip install kaggle pandas\n",
    "\n",
    "#!pip install pandas numpy seaborn matplotlib plotly scikit-learn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIpnCreUxxwx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import NMF\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Import \n",
    "- NOTE: Selecting more then 10,000 rows, so it can be filtered down to a more even distribution of 10,000, this is about the minimum required for equal subject groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvZFTN0lBVgL"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"arxiv_data/arxiv-metadata-oai-snapshot.json\", lines=True, nrows=180000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcessing\n",
    "\n",
    "- Your preprocessing code lumped into one code block\n",
    "- Unchanged, meant to be easily run, then collapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JuoMh2tE_El",
    "outputId": "ada15dfc-3f43-4a26-d4d1-8eeaa9927b18"
   },
   "outputs": [],
   "source": [
    "category_map = {\n",
    "    'astro-ph': 'Astrophysics',\n",
    "    'astro-ph.CO': 'Cosmology and Nongalactic Astrophysics',\n",
    "    'astro-ph.EP': 'Earth and Planetary Astrophysics',\n",
    "    'astro-ph.GA': 'Astrophysics of Galaxies',\n",
    "    'astro-ph.HE': 'High Energy Astrophysical Phenomena',\n",
    "    'astro-ph.IM': 'Instrumentation and Methods for Astrophysics',\n",
    "    'astro-ph.SR': 'Solar and Stellar Astrophysics',\n",
    "    'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "    'cond-mat.mes-hall': 'Mesoscale and Nanoscale Physics',\n",
    "    'cond-mat.mtrl-sci': 'Materials Science',\n",
    "    'cond-mat.other': 'Other Condensed Matter',\n",
    "    'cond-mat.quant-gas': 'Quantum Gases',\n",
    "    'cond-mat.soft': 'Soft Condensed Matter',\n",
    "    'cond-mat.stat-mech': 'Statistical Mechanics',\n",
    "    'cond-mat.str-el': 'Strongly Correlated Electrons',\n",
    "    'cond-mat.supr-con': 'Superconductivity',\n",
    "    'cs.AI': 'Artificial Intelligence',\n",
    "    'cs.AR': 'Hardware Architecture',\n",
    "    'cs.CC': 'Computational Complexity',\n",
    "    'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "    'cs.CG': 'Computational Geometry',\n",
    "    'cs.CL': 'Computation and Language',\n",
    "    'cs.CR': 'Cryptography and Security',\n",
    "    'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "    'cs.CY': 'Computers and Society',\n",
    "    'cs.DB': 'Databases',\n",
    "    'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "    'cs.DL': 'Digital Libraries',\n",
    "    'cs.DM': 'Discrete Mathematics',\n",
    "    'cs.DS': 'Data Structures and Algorithms',\n",
    "    'cs.ET': 'Emerging Technologies',\n",
    "    'cs.FL': 'Formal Languages and Automata Theory',\n",
    "    'cs.GL': 'General Literature',\n",
    "    'cs.GR': 'Graphics',\n",
    "    'cs.GT': 'Computer Science and Game Theory',\n",
    "    'cs.HC': 'Human-Computer Interaction',\n",
    "    'cs.IR': 'Information Retrieval',\n",
    "    'cs.IT': 'Information Theory',\n",
    "    'cs.LG': 'Machine Learning',\n",
    "    'cs.LO': 'Logic in Computer Science',\n",
    "    'cs.MA': 'Multiagent Systems',\n",
    "    'cs.MM': 'Multimedia',\n",
    "    'cs.MS': 'Mathematical Software',\n",
    "    'cs.NA': 'Numerical Analysis',\n",
    "    'cs.NE': 'Neural and Evolutionary Computing',\n",
    "    'cs.NI': 'Networking and Internet Architecture',\n",
    "    'cs.OH': 'Other Computer Science',\n",
    "    'cs.OS': 'Operating Systems',\n",
    "    'cs.PF': 'Performance',\n",
    "    'cs.PL': 'Programming Languages',\n",
    "    'cs.RO': 'Robotics',\n",
    "    'cs.SC': 'Symbolic Computation',\n",
    "    'cs.SD': 'Sound',\n",
    "    'cs.SE': 'Software Engineering',\n",
    "    'cs.SI': 'Social and Information Networks',\n",
    "    'cs.SY': 'Systems and Control',\n",
    "    'econ.EM': 'Econometrics',\n",
    "    'eess.AS': 'Audio and Speech Processing',\n",
    "    'eess.IV': 'Image and Video Processing',\n",
    "    'eess.SP': 'Signal Processing',\n",
    "    'gr-qc': 'General Relativity and Quantum Cosmology',\n",
    "    'hep-ex': 'High Energy Physics - Experiment',\n",
    "    'hep-lat': 'High Energy Physics - Lattice',\n",
    "    'hep-ph': 'High Energy Physics - Phenomenology',\n",
    "    'hep-th': 'High Energy Physics - Theory',\n",
    "    'math.AC': 'Commutative Algebra',\n",
    "    'math.AG': 'Algebraic Geometry',\n",
    "    'math.AP': 'Analysis of PDEs',\n",
    "    'math.AT': 'Algebraic Topology',\n",
    "    'math.CA': 'Classical Analysis and ODEs',\n",
    "    'math.CO': 'Combinatorics',\n",
    "    'math.CT': 'Category Theory',\n",
    "    'math.CV': 'Complex Variables',\n",
    "    'math.DG': 'Differential Geometry',\n",
    "    'math.DS': 'Dynamical Systems',\n",
    "    'math.FA': 'Functional Analysis',\n",
    "    'math.GM': 'General Mathematics',\n",
    "    'math.GN': 'General Topology',\n",
    "    'math.GR': 'Group Theory',\n",
    "    'math.GT': 'Geometric Topology',\n",
    "    'math.HO': 'History and Overview',\n",
    "    'math.IT': 'Information Theory',\n",
    "    'math.KT': 'K-Theory and Homology',\n",
    "    'math.LO': 'Logic',\n",
    "    'math.MG': 'Metric Geometry',\n",
    "    'math.MP': 'Mathematical Physics',\n",
    "    'math.NA': 'Numerical Analysis',\n",
    "    'math.NT': 'Number Theory',\n",
    "    'math.OA': 'Operator Algebras',\n",
    "    'math.OC': 'Optimization and Control',\n",
    "    'math.PR': 'Probability',\n",
    "    'math.QA': 'Quantum Algebra',\n",
    "    'math.RA': 'Rings and Algebras',\n",
    "    'math.RT': 'Representation Theory',\n",
    "    'math.SG': 'Symplectic Geometry',\n",
    "    'math.SP': 'Spectral Theory',\n",
    "    'math.ST': 'Statistics Theory',\n",
    "    'math-ph': 'Mathematical Physics',\n",
    "    'nlin.AO': 'Adaptation and Self-Organizing Systems',\n",
    "    'nlin.CD': 'Chaotic Dynamics',\n",
    "    'nlin.CG': 'Cellular Automata and Lattice Gases',\n",
    "    'nlin.PS': 'Pattern Formation and Solitons',\n",
    "    'nlin.SI': 'Exactly Solvable and Integrable Systems',\n",
    "    'nucl-ex': 'Nuclear Experiment',\n",
    "    'nucl-th': 'Nuclear Theory',\n",
    "    'physics.acc-ph': 'Accelerator Physics',\n",
    "    'physics.ao-ph': 'Atmospheric and Oceanic Physics',\n",
    "    'physics.app-ph': 'Applied Physics',\n",
    "    'physics.atm-clus': 'Atomic and Molecular Clusters',\n",
    "    'physics.atom-ph': 'Atomic Physics',\n",
    "    'physics.bio-ph': 'Biological Physics',\n",
    "    'physics.chem-ph': 'Chemical Physics',\n",
    "    'physics.class-ph': 'Classical Physics',\n",
    "    'physics.comp-ph': 'Computational Physics',\n",
    "    'physics.data-an': 'Data Analysis, Statistics and Probability',\n",
    "    'physics.ed-ph': 'Physics Education',\n",
    "    'physics.flu-dyn': 'Fluid Dynamics',\n",
    "    'physics.gen-ph': 'General Physics',\n",
    "    'physics.geo-ph': 'Geophysics',\n",
    "    'physics.hist-ph': 'History and Philosophy of Physics',\n",
    "    'physics.ins-det': 'Instrumentation and Detectors',\n",
    "    'physics.med-ph': 'Medical Physics',\n",
    "    'physics.optics': 'Optics',\n",
    "    'physics.plasm-ph': 'Plasma Physics',\n",
    "    'physics.pop-ph': 'Popular Physics',\n",
    "    'physics.soc-ph': 'Physics and Society',\n",
    "    'physics.space-ph': 'Space Physics',\n",
    "    'q-bio.BM': 'Biomolecules',\n",
    "    'q-bio.CB': 'Cell Behavior',\n",
    "    'q-bio.GN': 'Genomics',\n",
    "    'q-bio.MN': 'Molecular Networks',\n",
    "    'q-bio.NC': 'Neurons and Cognition',\n",
    "    'q-bio.OT': 'Other Quantitative Biology',\n",
    "    'q-bio.PE': 'Populations and Evolution',\n",
    "    'q-bio.QM': 'Quantitative Methods',\n",
    "    'q-bio.SC': 'Subcellular Processes',\n",
    "    'q-bio.TO': 'Tissues and Organs',\n",
    "    'q-fin.CP': 'Computational Finance',\n",
    "    'q-fin.EC': 'Economics',\n",
    "    'q-fin.GN': 'General Finance',\n",
    "    'q-fin.MF': 'Mathematical Finance',\n",
    "    'q-fin.PM': 'Portfolio Management',\n",
    "    'q-fin.PR': 'Pricing of Securities',\n",
    "    'q-fin.RM': 'Risk Management',\n",
    "    'q-fin.ST': 'Statistical Finance',\n",
    "    'q-fin.TR': 'Trading and Market Microstructure',\n",
    "    'quant-ph': 'Quantum Physics',\n",
    "    'stat.AP': 'Applications',\n",
    "    'stat.CO': 'Computation',\n",
    "    'stat.ME': 'Methodology',\n",
    "    'stat.ML': 'Machine Learning',\n",
    "    'stat.OT': 'Other Statistics',\n",
    "    'stat.TH': 'Statistics Theory'\n",
    "}\n",
    "\n",
    "subject_map = {\n",
    "    \"Astrophysics & Cosmology\": [\"astro-ph\", \"astro-ph.CO\", \"astro-ph.EP\", \"astro-ph.GA\", \"astro-ph.HE\", \"astro-ph.IM\", \"astro-ph.SR\"],\n",
    "    \"Condensed Matter Physics\": [\"cond-mat.dis-nn\", \"cond-mat.mes-hall\", \"cond-mat.mtrl-sci\", \"cond-mat.other\", \"cond-mat.quant-gas\", \"cond-mat.soft\", \"cond-mat.stat-mech\", \"cond-mat.str-el\", \"cond-mat.supr-con\"],\n",
    "    \"Computer Science\": [\"cs.AI\", \"cs.AR\", \"cs.CC\", \"cs.CE\", \"cs.CG\", \"cs.CL\", \"cs.CR\", \"cs.CV\", \"cs.CY\", \"cs.DB\", \"cs.DC\", \"cs.DL\", \"cs.DM\", \"cs.DS\", \"cs.ET\", \"cs.FL\", \"cs.GL\", \"cs.GR\", \"cs.GT\", \"cs.HC\", \"cs.IR\", \"cs.IT\", \"cs.LG\", \"cs.LO\", \"cs.MA\", \"cs.MM\", \"cs.MS\", \"cs.NA\", \"cs.NE\", \"cs.NI\", \"cs.OH\", \"cs.OS\", \"cs.PF\", \"cs.PL\", \"cs.RO\", \"cs.SC\", \"cs.SD\", \"cs.SE\", \"cs.SI\", \"cs.SY\"],\n",
    "    \"Econometrics & Finance\": [\"econ.EM\", \"q-fin.CP\", \"q-fin.EC\", \"q-fin.GN\", \"q-fin.MF\", \"q-fin.PM\", \"q-fin.PR\", \"q-fin.RM\", \"q-fin.ST\", \"q-fin.TR\"],\n",
    "    \"Electrical Engineering & Signal Processing\": [\"eess.AS\", \"eess.IV\", \"eess.SP\"],\n",
    "    \"General & Theoretical Physics\": [\"gr-qc\", \"hep-ex\", \"hep-lat\", \"hep-ph\", \"hep-th\", \"math-ph\", \"physics.acc-ph\", \"physics.ao-ph\", \"physics.app-ph\", \"physics.atm-clus\", \"physics.atom-ph\", \"physics.bio-ph\", \"physics.chem-ph\", \"physics.class-ph\", \"physics.comp-ph\", \"physics.data-an\", \"physics.ed-ph\", \"physics.flu-dyn\", \"physics.gen-ph\", \"physics.geo-ph\", \"physics.hist-ph\", \"physics.ins-det\", \"physics.med-ph\", \"physics.optics\", \"physics.plasm-ph\", \"physics.pop-ph\", \"physics.soc-ph\", \"physics.space-ph\", \"quant-ph\"],\n",
    "    \"Mathematics\": [\"math.AC\", \"math.AG\", \"math.AP\", \"math.AT\", \"math.CA\", \"math.CO\", \"math.CT\", \"math.CV\", \"math.DG\", \"math.DS\", \"math.FA\", \"math.GM\", \"math.GN\", \"math.GR\", \"math.GT\", \"math.HO\", \"math.IT\", \"math.KT\", \"math.LO\", \"math.MG\", \"math.MP\", \"math.NA\", \"math.NT\", \"math.OA\", \"math.OC\", \"math.PR\", \"math.QA\", \"math.RA\", \"math.RT\", \"math.SG\", \"math.SP\", \"math.ST\"],\n",
    "    \"Nonlinear Sciences\": [\"nlin.AO\", \"nlin.CD\", \"nlin.CG\", \"nlin.PS\", \"nlin.SI\"],\n",
    "    \"Nuclear Physics\": [\"nucl-ex\", \"nucl-th\"],\n",
    "    \"Quantitative Biology\": [\"q-bio.BM\", \"q-bio.CB\", \"q-bio.GN\", \"q-bio.MN\", \"q-bio.NC\", \"q-bio.OT\", \"q-bio.PE\", \"q-bio.QM\", \"q-bio.SC\", \"q-bio.TO\"],\n",
    "    \"Statistics & Data Science\": [\"stat.AP\", \"stat.CO\", \"stat.ME\", \"stat.ML\", \"stat.OT\", \"stat.TH\"]\n",
    "}\n",
    "\n",
    "# If there are multiple categories associated with the document, take the first one\n",
    "df['categories'] = df['categories'].str.split().str[0]\n",
    "\n",
    "# Add subjects instead of the category\n",
    "category_to_subject = {}\n",
    "for subject, categories in subject_map.items():\n",
    "    for category in categories:\n",
    "        category_to_subject[category] = subject\n",
    "\n",
    "# Apply the corrected mapping\n",
    "df['mapped_categories'] = df['categories'].map(category_map)\n",
    "df['subject_map'] = df['categories'].map(category_to_subject)\n",
    "df['mapped_categories'] = df['mapped_categories'].fillna('Unknown Category') #should not exist\n",
    "df['subject_map'] = df['subject_map'].fillna('Unknown Subject')  #should not exist\n",
    "\n",
    "# Print unique subject mappings (ie. without duplicates)\n",
    "df[['categories', 'mapped_categories', 'subject_map']].drop_duplicates().head(5)\n",
    "\n",
    "\n",
    "\n",
    "# Drop rows where 'title' or 'abstract' are missing\n",
    "df_cleaned = df.dropna(subset=['title', 'abstract'])\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to 'title' and 'abstract' columns\n",
    "df_cleaned['title'] = df_cleaned['title'].apply(clean_text)\n",
    "df_cleaned['abstract'] = df_cleaned['abstract'].apply(clean_text)\n",
    "\n",
    "# Remove duplicate rows based on 'title' and 'abstract'\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['title', 'abstract'])\n",
    "\n",
    "# Convert the update date to a proper datetime format\n",
    "df_cleaned['update_date'] = pd.to_datetime(df_cleaned['update_date'], errors='coerce')\n",
    "\n",
    "# Extract the year from the update date\n",
    "df_cleaned['year'] = df_cleaned['update_date'].dt.year\n",
    "\n",
    "# Exclude rows where the category is \"Unknown Category\"\n",
    "df_cleaned = df_cleaned[df_cleaned['mapped_categories'] != 'Unknown Category']\n",
    "\n",
    "\"\"\"\n",
    "Reformats Author Data Into Clean List\n",
    "\"\"\"\n",
    "def clean_contributors(author_string):\n",
    "\n",
    "    #Remove Anything In Brackets\n",
    "    s = re.sub(r'\\([^)]*\\)', '', author_string)\n",
    "\n",
    "    #Remove All \"and\"\n",
    "    s = re.sub(r'\\band\\b', '', s, flags=re.IGNORECASE)\n",
    "\n",
    "    #Remove All \"\\n\"\n",
    "    s = s.replace('\\n', '') \n",
    "\n",
    "    #Split And Trim\n",
    "    return [part.strip() for part in s.split(',') if part.strip()]\n",
    "    \n",
    "#Cleans The Author Columns\n",
    "df_cleaned['authors_parsed'] = df_cleaned['authors'].apply(clean_contributors)\n",
    "df_cleaned = df_cleaned.drop(columns='authors')\n",
    "\n",
    "#Store Count\n",
    "df_cleaned['contributors'] = df['authors_parsed'].apply(len)\n",
    "\n",
    "#Order \n",
    "df_cleaned = df_cleaned.sort_values(by='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Filtering\n",
    "\n",
    "- NOTE: selecting a subset of ~10,000 approximately equally subject distributed rows\n",
    "- Focuses on equalizing Subjects, not categories, as the distribution of Categories within Subject represent a valid distribution of practical effort within that Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting Categories\n",
    "key_categories = df_cleaned['subject_map'].nunique()\n",
    "\n",
    "#Counting Sample Per Category We Want\n",
    "samples_per_category = int(10000 / key_categories)\n",
    "\n",
    "#Selecting These Samples\n",
    "df_cleaned = (\n",
    "    df_cleaned.groupby('subject_map')\n",
    "      .apply(lambda x: x.sample(n=min(samples_per_category, len(x)), random_state=42))\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Distribution\n",
    "- Highlighting the new more equalized distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "\n",
    "# Count categories\n",
    "counts_orig = df_cleaned['mapped_categories'].value_counts().reset_index()\n",
    "counts_orig.columns = ['Category', 'Count']\n",
    "\n",
    "counts_cond = df_cleaned['subject_map'].value_counts().reset_index()\n",
    "counts_cond.columns = ['Category', 'Count']\n",
    "\n",
    "# Plot original categories with hue workaround\n",
    "sns.barplot(data=counts_orig,x='Category',y='Count',hue='Category',palette='Set2',legend=False,ax=axes[0])\n",
    "axes[0].set_title('Distribution of Papers by Default Category')\n",
    "axes[0].set_xlabel('Category')\n",
    "axes[0].set_ylabel('Number of Papers')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Plot condensed categories with hue workaround\n",
    "sns.barplot(data=counts_cond,x='Category',y='Count', hue='Category',palette='Set2',legend=False,ax=axes[1])\n",
    "axes[1].set_title('Distribution of Papers by Condensed Category')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "- Code to utilize both BERT and Sentence Embedding (although only Sentence is used below)\n",
    "- Largely your same code, described by the existing heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models Used\n",
    "model_full_bert = 'bert-base-uncased'\n",
    "model_sentence = 'all-MiniLM-L6-v2'\n",
    "\n",
    "#Tokenizers And Models Required\n",
    "full_tokenizer = BertTokenizer.from_pretrained(model_full_bert, ignore_mismatched_sizes=True)\n",
    "full_model = BertModel.from_pretrained(model_full_bert, ignore_mismatched_sizes=True)\n",
    "sentence_model = SentenceTransformer(model_sentence)\n",
    "\n",
    "\"\"\"\n",
    "Your Original Method: Averaging Word Embeddings, Better For Finding Keyword Similarity\n",
    "\"\"\"\n",
    "def original_token_embedding(text):\n",
    "\n",
    "        inputs = full_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        with torch.no_grad(): outputs = full_model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings.numpy().flatten()\n",
    "\n",
    "\"\"\"\n",
    "Generates Embedding According To Chosen Method\n",
    "\"\"\"\n",
    "def generate_embeddings(method, applied_on):\n",
    "        if method == \"token\":\n",
    "                return applied_on.apply(original_token_embedding)\n",
    "        else:\n",
    "                return list(sentence_model.encode(applied_on.tolist(), batch_size=128, show_progress_bar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Initial Embeddings\n",
    "\n",
    "- Generating embeddings based on the Title and Abstract Of Each Paper\n",
    "- Unchanged from your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['title_embeddings'] = generate_embeddings('token', df_cleaned['title'])\n",
    "df_cleaned['abstract_embeddings'] = generate_embeddings('token', df_cleaned['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Embedding Layer: Recursive Aggregation\n",
    "\n",
    "- The embeddings generated above can be further refined by applying them through author focused aggregation, so as to better represent the additional nuance contained by the specific authorship patterns within the data.\n",
    "\n",
    "- Taking the initial ((title/abstract)) embeddings for each paper, by aggregating the embeddings of all papers contributed to by each author, utilizing the role of this author (led or not), the temperal point its publishing occured within their career, and number of contributors to generate an embedding representing that authors average \"area of expertise\" or \"unique skill\".\n",
    "\n",
    "- For each paper, an embedding can then be regenerated by aggregating this author layer, regenerating the \"content\" representation of each papers, as the aggregate of the author embeddings among authors who contributed within it, to incorporate varience and tendancies within author publishing, utilizing each authors role (led or not), ......the magnitude of their papers, \n",
    "\n",
    "\n",
    "- Importance: Allows papers to be brought together by their practical similarity in conjuction to just theoretical, actual occurances of varience in subject expertise and publishship, collaborations and career topic progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Each Individual Contribution (Paper, Author)\n",
    "df_contributions = (\n",
    "    df_cleaned\n",
    "    .assign(\n",
    "        contributors = df_cleaned[\"authors_parsed\"].apply(len)\n",
    "    )\n",
    "    .explode(\"authors_parsed\", ignore_index=True)\n",
    "    .rename(columns={\"authors_parsed\":\"contributor\"})\n",
    "    .assign(\n",
    "        is_lead = lambda d: d.apply(lambda row: any(w.lower() in re.split(r'[^A-Za-z]+', row['contributor'].lower()) for w in re.split(r'[^A-Za-z]+', row['submitter'].lower())), axis=1)\n",
    "    )\n",
    "    .loc[:, lambda d: ~d.columns.isin([\"authors_parsed\",\"unclean_contributor\", \"submitter\", \"authors\", \"comments\", \"journal-ref\", \"doi\", \"license\", \"versions\", \"update_date\", \"report-no\", \"categories\", \"title\", \"abstract\"])]\n",
    ")\n",
    "\n",
    "#Generating Each Individual Author\n",
    "df_authors = (\n",
    "    df_contributions\n",
    "    .groupby(\"contributor\")\n",
    "    .agg(\n",
    "        papers = ('id', 'count'),\n",
    "        earliest = ('year', 'min'),\n",
    "        latest = ('year', 'max'),\n",
    "        solo = ('contributors', lambda x: (x==1).sum()),\n",
    "        average_size = ('contributors', 'mean'),\n",
    "        mapped_category_count = ('mapped_categories', lambda x: x.dropna().nunique()),\n",
    "        subject_map_count = ('subject_map', lambda x: x.dropna().nunique())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "df_authors.head(1)\n",
    "\n",
    "\n",
    "#Merging Data To Make Next Steps Easier\n",
    "df_contributions = df_contributions.merge(df_authors[['contributor', 'earliest', 'latest', 'papers']], on='contributor', how='left')\n",
    "paper_sums = df_contributions.groupby('id')['papers'].sum().reset_index(name='total_papers')\n",
    "df_cleaned = df_cleaned.merge(paper_sums, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates The Average Embeddings For A Category Using The Weighted Papers Within It\n",
    "\"\"\"\n",
    "def paper_contribution_to_category(group, chosen_embedding):\n",
    "    return np.average(\n",
    "        np.stack(group[chosen_embedding].values),\n",
    "        axis = 0,\n",
    "        weights = np.sqrt(group['contributors'].values) * #Use Contributor\n",
    "                  np.sqrt(group['total_papers']/group['contributors']) #Higher Publishing experience\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "Generates The Average Author Embedding Using Weighted Papers They Contributed To\n",
    "\"\"\"\n",
    "def paper_contribution_to_author(group,chosen_embedding):\n",
    "    \n",
    "    duration = np.maximum(group['latest']-group['earliest'],1)\n",
    "    point_within = group['year']-group['earliest']\n",
    "\n",
    "    return np.average(\n",
    "        np.stack(group[chosen_embedding].values),\n",
    "        axis=0,\n",
    "        weights=  np.log(point_within/duration + 2)*   #Use Point In Career\n",
    "                   ( 1 + group['is_lead']*3 ) *   #Role In Paper\n",
    "                  (np.power(1 / group['contributors'], 0.5))  # Inverse Contributors \n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generates The Average Paper Embedding Usings Weighted Authors That Contributed To It\n",
    "\"\"\"\n",
    "def authors_contribution_to_paper(group, chosen_embedding):\n",
    "    return np.average(\n",
    "        np.stack(group[chosen_embedding].values),\n",
    "        axis=0,\n",
    "        weights=  ( 1 + group['is_lead']*3 ) * #More If Author Leading It\n",
    "                    np.sqrt(group['papers']) * #Move If Many Papers Made\n",
    "                    (1 - 0.9 * (group['papers'] == 1)) #Less If This Is Only Paper Made By Author (Dilutes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "A Method That Generates An Alternative Embedding For A Paper, By Generating An Embedding For Each Author, \n",
    "Then Aggregating Each Papers Contributors Embeddings \n",
    "\"\"\"\n",
    "def generate_alternative_embeddings(chosen_embeddings, contributions):\n",
    "\n",
    "    #Generate Author Embedding\n",
    "    author_embeddings = (\n",
    "        contributions\n",
    "        .groupby('contributor')\n",
    "        .apply(lambda group: paper_contribution_to_author(group,chosen_embeddings), include_groups=False)\n",
    "        .reset_index(name = \"author_\" + chosen_embeddings)\n",
    "    )\n",
    "\n",
    "    #Merge To Join This Data\n",
    "    contributions = contributions.merge(author_embeddings, on='contributor', how='left')\n",
    "\n",
    "    #Generate New Paper Embedding\n",
    "    alternative_embeddings = (\n",
    "        contributions\n",
    "        .groupby('id')\n",
    "        .apply(lambda group: authors_contribution_to_paper(group, \"author_\" + chosen_embeddings), include_groups=False)\n",
    "        .sort_index()\n",
    "        .reset_index(name= \"alternative_\" + chosen_embeddings)\n",
    "    )\n",
    "\n",
    "    #Returning New Embedding\n",
    "    return list(alternative_embeddings[\"alternative_\" + chosen_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Generating New Embeddings\n",
    "\n",
    "- Regenerating both the title, and abstract embeddings, using this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['alternative_title_embeddings'] =    generate_alternative_embeddings( chosen_embeddings=\"title_embeddings\",    contributions=df_contributions)\n",
    "df_cleaned['alternative_abstract_embeddings'] = generate_alternative_embeddings(chosen_embeddings=\"abstract_embeddings\", contributions=df_contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Category Cosine Similarity:\n",
    "\n",
    "- Very similar to code you had, except adding a second matrix for similarity of a second set of embeddings, then highlighting the differences (normalized so areas of interest or unique levels are variation are highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXnrGFTx2eNE",
    "outputId": "fafa6703-c4e4-4425-f11d-b76d2e6befea"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_similarity_matrix(title_col, abstract_col, df):\n",
    "    # Combine title and abstract embeddings\n",
    "    combined = 0.5 * (df[title_col] + df[abstract_col])\n",
    "    sim_matrix = cosine_similarity(np.stack(combined))\n",
    "    \n",
    "    cats = df['subject_map'].unique()\n",
    "    cat_sim = np.zeros((len(cats), len(cats)))\n",
    "    \n",
    "    for i, c1 in enumerate(cats):\n",
    "        i1 = df['subject_map'] == c1\n",
    "        for j, c2 in enumerate(cats):\n",
    "            i2 = df['subject_map'] == c2\n",
    "            cat_sim[i, j] = sim_matrix[np.ix_(i1, i2)].mean()\n",
    "\n",
    "    np.fill_diagonal(cat_sim, np.nan)\n",
    "\n",
    "    return pd.DataFrame(cat_sim, index=cats, columns=cats)\n",
    "\n",
    "\n",
    "\n",
    "def plot_similarity_matrix(ax, similarity, title):\n",
    "    sns.heatmap(similarity, ax=ax, annot=True)\n",
    "    ax.set_title(\"Cosine Similarity \" + title)\n",
    "    ax.set_xlabel(\"Category\")\n",
    "    ax.set_ylabel(\"Category\")\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(30, 10))\n",
    "\n",
    "#Original Method\n",
    "original_method = create_similarity_matrix('title_embeddings', 'abstract_embeddings', df_cleaned)\n",
    "plot_similarity_matrix(axes[0], original_method, \"Original Embeddings\")\n",
    "\n",
    "#Alternative Method\n",
    "alternative_method = create_similarity_matrix('alternative_title_embeddings', 'alternative_abstract_embeddings', df_cleaned)\n",
    "plot_similarity_matrix(axes[1], alternative_method, \"Alternative Embeddings\")\n",
    "\n",
    "#Normalized Differences\n",
    "diff = original_method - alternative_method\n",
    "diff = (diff - diff.mean()) / diff.std()\n",
    "plot_similarity_matrix(axes[2], diff, \"Normalized Difference\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates Group Based Embedding Average (NOT USED)\n",
    "\"\"\"\n",
    "def aggregate_groups(df, embedding, categories):\n",
    "    return (\n",
    "        df\n",
    "        .groupby(categories)\n",
    "        .apply(lambda group: paper_contribution_to_category(group, embedding), include_groups=False)\n",
    "        .reset_index(name='embeddings')\n",
    "        .sort_values(by=categories)\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
