{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhGIuusOwFlp"
   },
   "source": [
    "# **Project Overview: Cross-Disciplinary Paper Recommendations**\n",
    "\n",
    "This project leverages transformer-based models, specifically BERT (Bidirectional Encoder Representations from Transformers), to recommend academic research papers across disciplines. The system analyzes the semantic content of paper titles and abstracts, generating dense vector embeddings that capture meaning beyond keywords. These embeddings are then compared to identify conceptual similarities between papers.\n",
    "\n",
    "The primary objective is to foster cross-disciplinary connections by recommending research that may be relevant to researchers in different academic domains. For example, a paper in computer science may be suggested to a neuroscience researcher based on conceptual overlap in abstracts.\n",
    "\n",
    "## **The workflow consists of several key stages:**\n",
    "\n",
    "*   Data Preprocessing: Cleaning and transforming the raw data from the dataset, including removing duplicates and handling missing values.\n",
    "*   Embedding Generation: Using a pre-trained BERT model to generate dense embeddings for paper titles and abstracts, ensuring that each paper's semantic content is captured accurately.\n",
    "*   Similarity Computation: Calculating the cosine similarity between papers' embeddings to determine how closely related they are.\n",
    "*   Topic Modeling with LDA: Applying Latent Dirichlet Allocation (LDA) to uncover latent topics in research papers, helping to identify key themes across disciplines.\n",
    "*   Visualization: Using techniques like heatmaps, PCA, and dimensionality reduction methods to visualize the relationships and similarities between papers.\n",
    "\n",
    "By implementing these steps, this project aims to not only recommend relevant papers but also uncover potential intersections between research areas that can lead to novel discoveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23214,
     "status": "ok",
     "timestamp": 1742781799195,
     "user": {
      "displayName": "Dafna Matsegora",
      "userId": "08627055149929474058"
     },
     "user_tz": 240
    },
    "id": "dHpnfhRz4hZQ",
    "outputId": "2db63b57-707a-491f-9774-7148dee6b4bf"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade --force-reinstall nltk\n",
    "#!pip install torch\n",
    "#!pip install transformers==4.28.1\n",
    "#!pip install sentence-transformers==2.2.2\n",
    "#!pip install huggingface_hub==0.13.4\n",
    "#!pip install deepspeed\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIpnCreUxxwx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "#NOTE: \n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aAkffCBylD8"
   },
   "source": [
    "### Download and Load Arxiv Dataset\n",
    "Download and extract the Arxiv dataset from Kaggle, then load and display the data using pandas.\n",
    "\n",
    "- This dataset includes titles, abstracts, authors, and categories for academic papers across fields like\n",
    "  computer science, physics, and math.\n",
    "\n",
    "- Since the full dataset is extremely large, 10,000 rows are used to make processing and analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvZFTN0lBVgL"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Download arxiv zip file\n",
    "#!kaggle datasets download -d cornell-university/arxiv --force\n",
    "\n",
    "# Ensure zip file is in loaded directory\n",
    "print(\"Files in current directory:\", os.listdir())\n",
    "\n",
    "# Extract the dataset\n",
    "with zipfile.ZipFile(\"arxiv.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"arxiv_data\")\n",
    "\n",
    "# List extracted files\n",
    "print(\"Extracted files:\", os.listdir(\"arxiv_data\"))\n",
    "\n",
    "# Read the first 10,000 rows\n",
    "df = pd.read_json(\"arxiv_data/arxiv-metadata-oai-snapshot.json\", lines=True, nrows=10000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFIi3RUXyuxu"
   },
   "source": [
    "### Mapping Category Labels and Assigning Subject Areas\n",
    "The dataset uses abbreviated category labels (e.g., cs.LG for Machine Learning), which are compact but not always intuitive for analysis. To improve interpretability, these codes are mapped to their full descriptive names. Additionally, to better group research areas, a subject mapping process is applied. This categorizes papers into broader subject areas, such as Computer Science, Mathematics, and Physics, based on their assigned categories.\n",
    "\n",
    "Each paper's primary category is extracted and mapped to its full name using a predefined dictionary. Afterward, papers are assigned a subject label based on a second mapping that consolidates related categories. If a category is not found in the predefined mappings, it is labeled as \"Unknown Category\" or \"Unknown Subject\", ensuring data consistency.\n",
    "\n",
    "By applying this mapping, we facilitate more effective filtering, visualization, and cross-disciplinary analysis of research trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JuoMh2tE_El",
    "outputId": "ada15dfc-3f43-4a26-d4d1-8eeaa9927b18"
   },
   "outputs": [],
   "source": [
    "# Dictionary of categories and full names to be mapped\n",
    "category_map = {\n",
    "    'astro-ph': 'Astrophysics',\n",
    "    'astro-ph.CO': 'Cosmology and Nongalactic Astrophysics',\n",
    "    'astro-ph.EP': 'Earth and Planetary Astrophysics',\n",
    "    'astro-ph.GA': 'Astrophysics of Galaxies',\n",
    "    'astro-ph.HE': 'High Energy Astrophysical Phenomena',\n",
    "    'astro-ph.IM': 'Instrumentation and Methods for Astrophysics',\n",
    "    'astro-ph.SR': 'Solar and Stellar Astrophysics',\n",
    "    'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "    'cond-mat.mes-hall': 'Mesoscale and Nanoscale Physics',\n",
    "    'cond-mat.mtrl-sci': 'Materials Science',\n",
    "    'cond-mat.other': 'Other Condensed Matter',\n",
    "    'cond-mat.quant-gas': 'Quantum Gases',\n",
    "    'cond-mat.soft': 'Soft Condensed Matter',\n",
    "    'cond-mat.stat-mech': 'Statistical Mechanics',\n",
    "    'cond-mat.str-el': 'Strongly Correlated Electrons',\n",
    "    'cond-mat.supr-con': 'Superconductivity',\n",
    "    'cs.AI': 'Artificial Intelligence',\n",
    "    'cs.AR': 'Hardware Architecture',\n",
    "    'cs.CC': 'Computational Complexity',\n",
    "    'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "    'cs.CG': 'Computational Geometry',\n",
    "    'cs.CL': 'Computation and Language',\n",
    "    'cs.CR': 'Cryptography and Security',\n",
    "    'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "    'cs.CY': 'Computers and Society',\n",
    "    'cs.DB': 'Databases',\n",
    "    'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "    'cs.DL': 'Digital Libraries',\n",
    "    'cs.DM': 'Discrete Mathematics',\n",
    "    'cs.DS': 'Data Structures and Algorithms',\n",
    "    'cs.ET': 'Emerging Technologies',\n",
    "    'cs.FL': 'Formal Languages and Automata Theory',\n",
    "    'cs.GL': 'General Literature',\n",
    "    'cs.GR': 'Graphics',\n",
    "    'cs.GT': 'Computer Science and Game Theory',\n",
    "    'cs.HC': 'Human-Computer Interaction',\n",
    "    'cs.IR': 'Information Retrieval',\n",
    "    'cs.IT': 'Information Theory',\n",
    "    'cs.LG': 'Machine Learning',\n",
    "    'cs.LO': 'Logic in Computer Science',\n",
    "    'cs.MA': 'Multiagent Systems',\n",
    "    'cs.MM': 'Multimedia',\n",
    "    'cs.MS': 'Mathematical Software',\n",
    "    'cs.NA': 'Numerical Analysis',\n",
    "    'cs.NE': 'Neural and Evolutionary Computing',\n",
    "    'cs.NI': 'Networking and Internet Architecture',\n",
    "    'cs.OH': 'Other Computer Science',\n",
    "    'cs.OS': 'Operating Systems',\n",
    "    'cs.PF': 'Performance',\n",
    "    'cs.PL': 'Programming Languages',\n",
    "    'cs.RO': 'Robotics',\n",
    "    'cs.SC': 'Symbolic Computation',\n",
    "    'cs.SD': 'Sound',\n",
    "    'cs.SE': 'Software Engineering',\n",
    "    'cs.SI': 'Social and Information Networks',\n",
    "    'cs.SY': 'Systems and Control',\n",
    "    'econ.EM': 'Econometrics',\n",
    "    'eess.AS': 'Audio and Speech Processing',\n",
    "    'eess.IV': 'Image and Video Processing',\n",
    "    'eess.SP': 'Signal Processing',\n",
    "    'gr-qc': 'General Relativity and Quantum Cosmology',\n",
    "    'hep-ex': 'High Energy Physics - Experiment',\n",
    "    'hep-lat': 'High Energy Physics - Lattice',\n",
    "    'hep-ph': 'High Energy Physics - Phenomenology',\n",
    "    'hep-th': 'High Energy Physics - Theory',\n",
    "    'math.AC': 'Commutative Algebra',\n",
    "    'math.AG': 'Algebraic Geometry',\n",
    "    'math.AP': 'Analysis of PDEs',\n",
    "    'math.AT': 'Algebraic Topology',\n",
    "    'math.CA': 'Classical Analysis and ODEs',\n",
    "    'math.CO': 'Combinatorics',\n",
    "    'math.CT': 'Category Theory',\n",
    "    'math.CV': 'Complex Variables',\n",
    "    'math.DG': 'Differential Geometry',\n",
    "    'math.DS': 'Dynamical Systems',\n",
    "    'math.FA': 'Functional Analysis',\n",
    "    'math.GM': 'General Mathematics',\n",
    "    'math.GN': 'General Topology',\n",
    "    'math.GR': 'Group Theory',\n",
    "    'math.GT': 'Geometric Topology',\n",
    "    'math.HO': 'History and Overview',\n",
    "    'math.IT': 'Information Theory',\n",
    "    'math.KT': 'K-Theory and Homology',\n",
    "    'math.LO': 'Logic',\n",
    "    'math.MG': 'Metric Geometry',\n",
    "    'math.MP': 'Mathematical Physics',\n",
    "    'math.NA': 'Numerical Analysis',\n",
    "    'math.NT': 'Number Theory',\n",
    "    'math.OA': 'Operator Algebras',\n",
    "    'math.OC': 'Optimization and Control',\n",
    "    'math.PR': 'Probability',\n",
    "    'math.QA': 'Quantum Algebra',\n",
    "    'math.RA': 'Rings and Algebras',\n",
    "    'math.RT': 'Representation Theory',\n",
    "    'math.SG': 'Symplectic Geometry',\n",
    "    'math.SP': 'Spectral Theory',\n",
    "    'math.ST': 'Statistics Theory',\n",
    "    'math-ph': 'Mathematical Physics',\n",
    "    'nlin.AO': 'Adaptation and Self-Organizing Systems',\n",
    "    'nlin.CD': 'Chaotic Dynamics',\n",
    "    'nlin.CG': 'Cellular Automata and Lattice Gases',\n",
    "    'nlin.PS': 'Pattern Formation and Solitons',\n",
    "    'nlin.SI': 'Exactly Solvable and Integrable Systems',\n",
    "    'nucl-ex': 'Nuclear Experiment',\n",
    "    'nucl-th': 'Nuclear Theory',\n",
    "    'physics.acc-ph': 'Accelerator Physics',\n",
    "    'physics.ao-ph': 'Atmospheric and Oceanic Physics',\n",
    "    'physics.app-ph': 'Applied Physics',\n",
    "    'physics.atm-clus': 'Atomic and Molecular Clusters',\n",
    "    'physics.atom-ph': 'Atomic Physics',\n",
    "    'physics.bio-ph': 'Biological Physics',\n",
    "    'physics.chem-ph': 'Chemical Physics',\n",
    "    'physics.class-ph': 'Classical Physics',\n",
    "    'physics.comp-ph': 'Computational Physics',\n",
    "    'physics.data-an': 'Data Analysis, Statistics and Probability',\n",
    "    'physics.ed-ph': 'Physics Education',\n",
    "    'physics.flu-dyn': 'Fluid Dynamics',\n",
    "    'physics.gen-ph': 'General Physics',\n",
    "    'physics.geo-ph': 'Geophysics',\n",
    "    'physics.hist-ph': 'History and Philosophy of Physics',\n",
    "    'physics.ins-det': 'Instrumentation and Detectors',\n",
    "    'physics.med-ph': 'Medical Physics',\n",
    "    'physics.optics': 'Optics',\n",
    "    'physics.plasm-ph': 'Plasma Physics',\n",
    "    'physics.pop-ph': 'Popular Physics',\n",
    "    'physics.soc-ph': 'Physics and Society',\n",
    "    'physics.space-ph': 'Space Physics',\n",
    "    'q-bio.BM': 'Biomolecules',\n",
    "    'q-bio.CB': 'Cell Behavior',\n",
    "    'q-bio.GN': 'Genomics',\n",
    "    'q-bio.MN': 'Molecular Networks',\n",
    "    'q-bio.NC': 'Neurons and Cognition',\n",
    "    'q-bio.OT': 'Other Quantitative Biology',\n",
    "    'q-bio.PE': 'Populations and Evolution',\n",
    "    'q-bio.QM': 'Quantitative Methods',\n",
    "    'q-bio.SC': 'Subcellular Processes',\n",
    "    'q-bio.TO': 'Tissues and Organs',\n",
    "    'q-fin.CP': 'Computational Finance',\n",
    "    'q-fin.EC': 'Economics',\n",
    "    'q-fin.GN': 'General Finance',\n",
    "    'q-fin.MF': 'Mathematical Finance',\n",
    "    'q-fin.PM': 'Portfolio Management',\n",
    "    'q-fin.PR': 'Pricing of Securities',\n",
    "    'q-fin.RM': 'Risk Management',\n",
    "    'q-fin.ST': 'Statistical Finance',\n",
    "    'q-fin.TR': 'Trading and Market Microstructure',\n",
    "    'quant-ph': 'Quantum Physics',\n",
    "    'stat.AP': 'Applications',\n",
    "    'stat.CO': 'Computation',\n",
    "    'stat.ME': 'Methodology',\n",
    "    'stat.ML': 'Machine Learning',\n",
    "    'stat.OT': 'Other Statistics',\n",
    "    'stat.TH': 'Statistics Theory'\n",
    "}\n",
    "\n",
    "subject_map = {\n",
    "    \"Astrophysics & Cosmology\": [\"astro-ph\", \"astro-ph.CO\", \"astro-ph.EP\", \"astro-ph.GA\", \"astro-ph.HE\", \"astro-ph.IM\", \"astro-ph.SR\"],\n",
    "    \"Condensed Matter Physics\": [\"cond-mat.dis-nn\", \"cond-mat.mes-hall\", \"cond-mat.mtrl-sci\", \"cond-mat.other\", \"cond-mat.quant-gas\", \"cond-mat.soft\", \"cond-mat.stat-mech\", \"cond-mat.str-el\", \"cond-mat.supr-con\"],\n",
    "    \"Computer Science\": [\"cs.AI\", \"cs.AR\", \"cs.CC\", \"cs.CE\", \"cs.CG\", \"cs.CL\", \"cs.CR\", \"cs.CV\", \"cs.CY\", \"cs.DB\", \"cs.DC\", \"cs.DL\", \"cs.DM\", \"cs.DS\", \"cs.ET\", \"cs.FL\", \"cs.GL\", \"cs.GR\", \"cs.GT\", \"cs.HC\", \"cs.IR\", \"cs.IT\", \"cs.LG\", \"cs.LO\", \"cs.MA\", \"cs.MM\", \"cs.MS\", \"cs.NA\", \"cs.NE\", \"cs.NI\", \"cs.OH\", \"cs.OS\", \"cs.PF\", \"cs.PL\", \"cs.RO\", \"cs.SC\", \"cs.SD\", \"cs.SE\", \"cs.SI\", \"cs.SY\"],\n",
    "    \"Econometrics & Finance\": [\"econ.EM\", \"q-fin.CP\", \"q-fin.EC\", \"q-fin.GN\", \"q-fin.MF\", \"q-fin.PM\", \"q-fin.PR\", \"q-fin.RM\", \"q-fin.ST\", \"q-fin.TR\"],\n",
    "    \"Electrical Engineering & Signal Processing\": [\"eess.AS\", \"eess.IV\", \"eess.SP\"],\n",
    "    \"General & Theoretical Physics\": [\"gr-qc\", \"hep-ex\", \"hep-lat\", \"hep-ph\", \"hep-th\", \"math-ph\", \"physics.acc-ph\", \"physics.ao-ph\", \"physics.app-ph\", \"physics.atm-clus\", \"physics.atom-ph\", \"physics.bio-ph\", \"physics.chem-ph\", \"physics.class-ph\", \"physics.comp-ph\", \"physics.data-an\", \"physics.ed-ph\", \"physics.flu-dyn\", \"physics.gen-ph\", \"physics.geo-ph\", \"physics.hist-ph\", \"physics.ins-det\", \"physics.med-ph\", \"physics.optics\", \"physics.plasm-ph\", \"physics.pop-ph\", \"physics.soc-ph\", \"physics.space-ph\", \"quant-ph\"],\n",
    "    \"Mathematics\": [\"math.AC\", \"math.AG\", \"math.AP\", \"math.AT\", \"math.CA\", \"math.CO\", \"math.CT\", \"math.CV\", \"math.DG\", \"math.DS\", \"math.FA\", \"math.GM\", \"math.GN\", \"math.GR\", \"math.GT\", \"math.HO\", \"math.IT\", \"math.KT\", \"math.LO\", \"math.MG\", \"math.MP\", \"math.NA\", \"math.NT\", \"math.OA\", \"math.OC\", \"math.PR\", \"math.QA\", \"math.RA\", \"math.RT\", \"math.SG\", \"math.SP\", \"math.ST\"],\n",
    "    \"Nonlinear Sciences\": [\"nlin.AO\", \"nlin.CD\", \"nlin.CG\", \"nlin.PS\", \"nlin.SI\"],\n",
    "    \"Nuclear Physics\": [\"nucl-ex\", \"nucl-th\"],\n",
    "    \"Quantitative Biology\": [\"q-bio.BM\", \"q-bio.CB\", \"q-bio.GN\", \"q-bio.MN\", \"q-bio.NC\", \"q-bio.OT\", \"q-bio.PE\", \"q-bio.QM\", \"q-bio.SC\", \"q-bio.TO\"],\n",
    "    \"Statistics & Data Science\": [\"stat.AP\", \"stat.CO\", \"stat.ME\", \"stat.ML\", \"stat.OT\", \"stat.TH\"]\n",
    "}\n",
    "\n",
    "# If there are multiple categories associated with the document, take the first one\n",
    "df['categories'] = df['categories'].str.split().str[0]\n",
    "\n",
    "# Add subjects instead of the category\n",
    "category_to_subject = {}\n",
    "for subject, categories in subject_map.items():\n",
    "    for category in categories:\n",
    "        category_to_subject[category] = subject\n",
    "\n",
    "# Apply the corrected mapping\n",
    "df['mapped_categories'] = df['categories'].map(category_map)\n",
    "df['subject_map'] = df['categories'].map(category_to_subject)\n",
    "df['mapped_categories'] = df['mapped_categories'].fillna('Unknown Category') #should not exist\n",
    "df['subject_map'] = df['subject_map'].fillna('Unknown Subject')  #should not exist\n",
    "\n",
    "# Print unique subject mappings (ie. without duplicates)\n",
    "df[['categories', 'mapped_categories', 'subject_map']].drop_duplicates().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0J24uHtzL0W"
   },
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "The following step involves cleaning and preprocessing the dataset by addressing missing values, standardizing text, and removing duplicate entries.\n",
    "\n",
    "The significance of this step is as follows:\n",
    "\n",
    "- Some records may have missing titles or abstracts, which are crucial for meaningful analysis. These rows are removed to ensure the dataset contains only complete and usable information.\n",
    "- Text data often contains extra spaces, punctuation marks, and inconsistencies in casing, which can hinder natural language processing (NLP) tasks. Standardizing the text ensures it is in a consistent and analyzable format.\n",
    "- Duplicate records, which contain identical titles and abstracts, are eliminated to prevent redundancy and maintain the integrity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mb6EnLLbGTmR",
    "outputId": "bf3302e2-45bd-428b-da02-b84c00bac98d"
   },
   "outputs": [],
   "source": [
    "# Drop rows where 'title' or 'abstract' are missing\n",
    "df_cleaned = df.dropna(subset=['title', 'abstract'])\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to 'title' and 'abstract' columns\n",
    "df_cleaned['title'] = df_cleaned['title'].apply(clean_text)\n",
    "df_cleaned['abstract'] = df_cleaned['abstract'].apply(clean_text)\n",
    "\n",
    "# Remove duplicate rows based on 'title' and 'abstract'\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['title', 'abstract'])\n",
    "\n",
    "# Display the cleaned dataframe shape\n",
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Cleaning And Reformatting Authorship Data\n",
    "\n",
    "- Additional piece of formatting that works to clean the messily stored author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reformats Author Data Into Clean List\n",
    "\"\"\"\n",
    "def clean_contributors(author_string):\n",
    "\n",
    "    #Remove Anything In Brackets\n",
    "    s = re.sub(r'\\([^)]*\\)', '', author_string)\n",
    "\n",
    "    #Remove All \"and\"\n",
    "    s = re.sub(r'\\band\\b', '', s, flags=re.IGNORECASE)\n",
    "\n",
    "    #Remove All \"\\n\"\n",
    "    s = s.replace('\\n', '') \n",
    "\n",
    "    #Split And Trim\n",
    "    return [part.strip() for part in s.split(',') if part.strip()]\n",
    "    \n",
    "#Cleans The Author Columns\n",
    "df_cleaned['authors_parsed'] = df_cleaned['authors'].apply(clean_contributors)\n",
    "df_cleaned = df_cleaned.drop(columns='authors')\n",
    "\n",
    "#Store Count\n",
    "df_cleaned['contributors'] = df['authors_parsed'].apply(len)\n",
    "\n",
    "#Order \n",
    "df_cleaned = df_cleaned.sort_values(by='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIs3N8PS75qu"
   },
   "source": [
    "### Distribution of Research\n",
    "\n",
    "This code visualizes the distribution of research papers in the dataset both before and after category mapping.\n",
    "\n",
    "First, it plots the distribution based on the default categories assigned by arXiv.This helps us see how papers are initially distributed across various categories.\n",
    "\n",
    "Next, it shows the distribution after condensing the categories using a custom mapping.This step results in clearer visualizations, as there are more papers in each category, and it resolves the issue of 'unknown' categories by consolidating them into defined ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of papers in each category to see how the dataset is distributed\n",
    "category_counts_original = df_cleaned['mapped_categories'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=category_counts_original.index, y=category_counts_original.values,palette='Set2')\n",
    "plt.title('Distribution of Papers by Default Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "category_counts_condensed = df_cleaned['subject_map'].value_counts()\n",
    "\n",
    "# Plot for condensed category map\n",
    "categories = sorted(df_cleaned['mapped_categories'].unique())\n",
    "subjects = sorted(df_cleaned['subject_map'].unique())\n",
    "\n",
    "# Define a fixed color palette\n",
    "palette = sns.color_palette(\"tab10\", max(len(categories), len(subjects)))\n",
    "category_counts = df_cleaned['subject_map'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=category_counts_condensed.index, y=category_counts_condensed.values, palette='Set2',)\n",
    "plt.title('Distribution of Papers by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KTldEQ69Yw4"
   },
   "source": [
    "### Processing Dates and Visualizing Publication Trends\n",
    "This code processes the dataset by converting the update date to a proper datetime format and extracting the year. It then filters out the \"Unknown Category\" rows. The plot_percentage_trends function calculates the percentage of papers published each year per subject category and visualizes these trends over time using line plots. The goal is to show how different research areas have evolved over the years in terms of their publication volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQNrB8--91o0"
   },
   "outputs": [],
   "source": [
    "# Convert the update date to a proper datetime format\n",
    "df_cleaned['update_date'] = pd.to_datetime(df_cleaned['update_date'], errors='coerce')\n",
    "\n",
    "# Extract the year from the update date\n",
    "df_cleaned['year'] = df_cleaned['update_date'].dt.year\n",
    "\n",
    "# Exclude rows where the category is \"Unknown Category\"\n",
    "df_filtered = df_cleaned[df_cleaned['mapped_categories'] != 'Unknown Category']\n",
    "\n",
    "# Split the dataset into pre-2010 and post-2010 categories for analysis\n",
    "def plot_percentage_trends(df, title):\n",
    "    # Calculate the total number of papers per year\n",
    "    total_papers_per_year = df.groupby('year').size()\n",
    "\n",
    "    # Count papers per subject and year\n",
    "    category_trends = df.groupby(['subject_map', 'year']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Convert counts to percentages\n",
    "    category_percentage = category_trends.div(total_papers_per_year, axis=1)\n",
    "\n",
    "    # Plot the temporal trends for each category as percentages\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for category in category_percentage.index:\n",
    "        sns.lineplot(x=category_percentage.columns, y=category_percentage.loc[category], marker='o', label=category, palette='Set2')\n",
    "\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Percentage of Papers Published', fontsize=12)\n",
    "    plt.legend(title='Subject Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot temporal trends by category as percentages\n",
    "plot_percentage_trends(df_filtered, 'Percentage of Papers Published by Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWeijuMI__Mh"
   },
   "source": [
    "### Summary Statistics for Category Distribution\n",
    "The following code calculates and visualizes summary statistics for both the original categories ('mapped_categories') and the condensed categories ('subject_map'). It provides insights into the distribution of papers across categories, highlighting the most and least frequent categories, as well as the mean and standard deviation of papers per category. These statistics are important for understanding the dataset's structure and identifying potential imbalances between categories, which can guide further analysis and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxDA6gVx_6kD"
   },
   "outputs": [],
   "source": [
    "# Summary statistics for default categories\n",
    "num_categories = category_counts_original.shape[0]\n",
    "most_frequent_category = category_counts_original.idxmax()\n",
    "most_frequent_count = category_counts_original.max()\n",
    "least_frequent_category = category_counts_original.idxmin()\n",
    "least_frequent_count = category_counts_original.min()\n",
    "mean_papers_per_category = category_counts_original.mean()\n",
    "std_dev_papers_per_category = category_counts_original.std()\n",
    "\n",
    "# Prepare the stats for the table\n",
    "summary_stats_default = [\n",
    "    [\"Total Categories\", num_categories],\n",
    "    [\"Most Frequent Category\",most_frequent_category],\n",
    "    [\"Papers in Most Frequent Category\", most_frequent_count],\n",
    "    [\"Least Frequent Category\",  least_frequent_category],\n",
    "    [\"Papers in Least Frequent Category\",least_frequent_count],\n",
    "    [\"Mean Papers Per Category\",f\"{mean_papers_per_category:.2f}\"],\n",
    "    [\"Std Dev of Papers per Category\", f\"{std_dev_papers_per_category:.2f}\"]\n",
    "]\n",
    "\n",
    "# Generate Table\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=summary_stats_default, colLabels=[\"Statistic\", \"Value\"],cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.auto_set_column_width([0, 1])\n",
    "\n",
    "# Adjust row heights so table is less squished\n",
    "for (row, col),cell in table.get_celld().items():\n",
    "      cell.set_height(0.1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics condensed categories\n",
    "num_categories = category_counts_condensed.shape[0]\n",
    "most_frequent_category = category_counts_condensed.idxmax()\n",
    "most_frequent_count = category_counts_condensed.max()\n",
    "least_frequent_category = category_counts_condensed.idxmin()\n",
    "least_frequent_count = category_counts_condensed.min()\n",
    "mean_papers_per_category = category_counts_condensed.mean()\n",
    "std_dev_papers_per_category =category_counts_condensed.std()\n",
    "\n",
    "# Prepare the stats for the table\n",
    "summary_stats_condensed = [\n",
    "    [\"Total Categories\", num_categories],\n",
    "    [\"Most Frequent Category\",most_frequent_category],\n",
    "    [\"Papers in Most Frequent Category\",most_frequent_count],\n",
    "    [\"Least Frequent Category\",least_frequent_category],\n",
    "    [\"Papers in Least Frequent Category\",least_frequent_count],\n",
    "    [\"Mean Papers per Category\", f\"{mean_papers_per_category:.2f}\"],\n",
    "    [\"Std Dev of Papers per Category\", f\"{std_dev_papers_per_category:.2f}\"]\n",
    "]\n",
    "\n",
    "# Table\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText = summary_stats_condensed, colLabels=[\"Statistic\", \"Value\"], cellLoc='center', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.auto_set_column_width([0, 1])\n",
    "\n",
    "# Adjust row heights\n",
    "for (row, col), cell in table.get_celld().items():\n",
    "      cell.set_height(0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8OqJ3fQ0K63"
   },
   "source": [
    "### Tokenization and Embedding Extraction\n",
    "\n",
    "This step utilizes a pre-trained BERT model to generate numerical embeddings for the titles and abstracts of research papers, enabling the use of these embeddings for various natural language processing (NLP) tasks.\n",
    "\n",
    "The significance of this step is as follows:\n",
    "\n",
    "- Converting Text into Numerical Data: Machine learning models perform optimally with numerical data. Textual data must be transformed into structured numerical representations to facilitate analysis. BERT embeddings provide a more sophisticated approach, capturing the contextual meaning of text more effectively than traditional methods such as TF-IDF.\n",
    "- Leveraging Pre-Trained Knowledge: Instead of training a model from scratch, we leverage the pre-trained BERT model (bert-base-uncased), which has been trained on a vast corpus of text data. This provides us with high-quality and context-aware embeddings, eliminating the need for domain-specific training.\n",
    "- Application to NLP Tasks: The embeddings generated from the titles and abstracts serve as the foundation for several NLP tasks, such as document similarity, topic modeling, research trend analysis, or even training classifiers for more advanced analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Addition To Embedding Extraction\n",
    "\n",
    "- Added a wrapper to existing method, to allow choice between one of 2 methods, (existing BERT or Sentence Based)\n",
    "- Currently Sentence Based (the current model at least) is smaller and quicker then BERT (although there are sentence based ones which are larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models Used\n",
    "model_full_bert = 'bert-base-uncased'\n",
    "model_sentence = 'all-MiniLM-L6-v2'\n",
    "\n",
    "#Tokenizers And Models Required\n",
    "full_tokenizer = BertTokenizer.from_pretrained(model_full_bert, ignore_mismatched_sizes=True)\n",
    "full_model = BertModel.from_pretrained(model_full_bert, ignore_mismatched_sizes=True)\n",
    "sentence_model = SentenceTransformer(model_sentence)\n",
    "\n",
    "\"\"\"\n",
    "Your Original Method: Averaging Word Embeddings, Better For Finding Keyword Similarity\n",
    "\"\"\"\n",
    "def original_token_embedding(text):\n",
    "\n",
    "        inputs = full_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        with torch.no_grad(): outputs = full_model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings.numpy().flatten()\n",
    "\n",
    "\"\"\"\n",
    "Generates Embedding According To Chosen Method\n",
    "\"\"\"\n",
    "def generate_embeddings(method, applied_on):\n",
    "        if method == \"token\":\n",
    "                return applied_on.apply(original_token_embedding)\n",
    "        else:\n",
    "                return list(sentence_model.encode(applied_on.tolist(), batch_size=128, show_progress_bar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Generating Embedding\n",
    "\n",
    "- In addition to what you had already, I added the compound utilizing the sentence method, to demonstrate using the other approach\n",
    "- It seems to me, that generating multiple and comparing these embeddings IE: (title, abstract, joined x token, sentence) might be interesting\n",
    "\n",
    "- ALTERNATIVE COMBOS CAN BE TRIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: sentence has progress bar\n",
    "df_cleaned['joined_embeddings'] = generate_embeddings('sentence', df_cleaned['title'] + \":\" + df_cleaned['abstract'])  \n",
    "df_cleaned['title_embeddings'] = generate_embeddings('token', df_cleaned['title'])\n",
    "df_cleaned['abstract_embeddings'] = generate_embeddings('token', df_cleaned['abstract'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Generating Alternative Method\n",
    "\n",
    "- So the previous embedding is based on language models, whats below is very similar but puts the embedding through a layer to incorporate tendencies for co-authorship and category overlap among authors\n",
    "- To do this, I start by making 2 new dataframes, one (df_contributions) to represent an authors contribution to a single paper, and a second (df_authors) to store aggregate information about each author\n",
    "- Some information is then merged, which is redundant space wise, but makes the code easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributions = (\n",
    "    df_cleaned\n",
    "    .assign(\n",
    "        contributors = df_cleaned[\"authors_parsed\"].apply(len)\n",
    "    )\n",
    "    .explode(\"authors_parsed\", ignore_index=True)\n",
    "    .rename(columns={\"authors_parsed\":\"contributor\"})\n",
    "    .assign(\n",
    "        is_lead = lambda d: d.apply(lambda row: any(w.lower() in re.split(r'[^A-Za-z]+', row['contributor'].lower()) for w in re.split(r'[^A-Za-z]+', row['submitter'].lower())), axis=1)\n",
    "    )\n",
    "    .loc[:, lambda d: ~d.columns.isin([\"authors_parsed\",\"unclean_contributor\", \"submitter\", \"authors\", \"comments\", \"journal-ref\", \"doi\", \"license\", \"versions\", \"update_date\", \"report-no\", \"categories\", \"title\", \"abstract\"])]\n",
    ")\n",
    "df_contributions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = (\n",
    "    df_contributions\n",
    "    .groupby(\"contributor\")\n",
    "    .agg(\n",
    "        papers = ('id', 'count'),\n",
    "        earliest = ('year', 'min'),\n",
    "        latest = ('year', 'max'),\n",
    "        solo = ('contributors', lambda x: (x==1).sum()),\n",
    "        average_size = ('contributors', 'mean'),\n",
    "        mapped_category_count = ('mapped_categories', lambda x: x.dropna().nunique()),\n",
    "        subject_map_count = ('subject_map', lambda x: x.dropna().nunique())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "df_authors.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributions = df_contributions.merge(df_authors[['contributor', 'earliest', 'latest', 'papers']], on='contributor', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_sums = df_contributions.groupby('id')['papers'].sum().reset_index(name='total_papers')\n",
    "df_cleaned = df_cleaned.merge(paper_sums, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Distribution Of Authorship\n",
    "\n",
    "- Displaying the distributions generated within the author and contribution dataframe\n",
    "- Most importantly, illustrating how many papers each authors writes (Paper Counts By Author), the number of solo papers written by each author (Single Paper Counts By Author), The number of categories each author has written papers within (Categories Per Authors), similarly the number of subjects (Subjects Per Author), The Counting Publishing Ratio, which stores for a given paper, the number of papers written by its authors divided by how many authors it has (metric for experience of authors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Generic Function To Plot The Distribution Of A Specified Column\n",
    "\"\"\"\n",
    "def plot_distribution(ax, data, title):\n",
    "    ax.hist(data, bins=350, edgecolor=\"black\", alpha=0.7)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(24, 5)) \n",
    "\n",
    "plot_distribution(axes[0,0], df_authors[\"papers\"], \"Paper Counts By Author\")\n",
    "plot_distribution(axes[0,1], df_authors[\"solo\"], \"Single Paper Counts By Author\")\n",
    "plot_distribution(axes[0,2], df_authors[\"mapped_category_count\"], \"Categories Per Author\")\n",
    "plot_distribution(axes[1,0], df_authors[\"subject_map_count\"], \"Subjects Per Author\")\n",
    "plot_distribution(axes[1,1], df_cleaned[\"total_papers\"]/df_cleaned['contributors'], \"Counting Publishing Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Alternative Embeddings\n",
    "\n",
    "- Now we have the new dataframes, we can start with the alternative method, essentially all it is, is taking the embeddings we generated above for individual papers, using the embeddings of all papers an author has ever worked on to generate an \"average\" embedding of subject for each author, then redefining each paper using the embedding of each author who worked on it, \n",
    "- There are 3 methods below, each with unique weighting\n",
    "- The first (paper_contribution_to_category) represents how each paper is weighted as part of its category definition, papers with higher importance are those with more contributors (more effort within the field) and those with higher publishing experience (total papers authors by them)\n",
    "\n",
    "- The second (paper_contribution_to_author) represents how each paper is weighted as part of its definition for it author, papers with higher importance are those that happen later in career, papers with which this author led, and papers with fewer contributors. \n",
    "\n",
    "- The third (authors_contribution_to_paper) represents the weight to which each author contributes to each papers new embedding, authors with higher importance is the author who led the paper, individuals who published many papers, and inversely authors who wrote only one paper (as their embedding equals the initial embedding)\n",
    "\n",
    "\n",
    "- Significance: The newly generated embedding, in addition to theoretical overlap takes into consideration real world coocurrance, and tendancy for practical overlap between categories, and range of papers within career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates The Average Embeddings For A Category Using The Weighted Papers Within It\n",
    "\"\"\"\n",
    "def paper_contribution_to_category(group, chosen_embedding):\n",
    "    return np.average(\n",
    "        np.stack(group[chosen_embedding].values),\n",
    "        axis = 0,\n",
    "        weights = np.sqrt(group['contributors'].values) * #Use Contributor\n",
    "                  np.sqrt(group['total_papers']/group['contributors']) #Higher Publishing experience\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "Generates The Average Author Embedding Using Weighted Papers They Contributed To\n",
    "\"\"\"\n",
    "def paper_contribution_to_author(group,chosen_embedding):\n",
    "    \n",
    "    duration = np.maximum(group['latest']-group['earliest'],1)\n",
    "    point_within = group['year']-group['earliest']\n",
    "\n",
    "    return np.average(\n",
    "        np.stack(group[chosen_embedding].values),\n",
    "        axis=0,\n",
    "        weights=  np.log(point_within/duration + 2)*   #Use Point In Career\n",
    "                   ( 1 + group['is_lead']*3 ) *   #Role In Paper\n",
    "                  (np.power(1 / group['contributors'], 0.5))  # Inverse Contributors \n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generates The Average Paper Embedding Usings Weighted Authors That Contributed To It\n",
    "\"\"\"\n",
    "def authors_contribution_to_paper(group, chosen_embedding):\n",
    "    return np.average(\n",
    "        np.stack(group[chosen_embedding].values),\n",
    "        axis=0,\n",
    "        weights=  ( 1 + group['is_lead']*3 ) * #More If Author Leading It\n",
    "                    np.sqrt(group['papers']) * #Move If Many Papers Made\n",
    "                    (1 - 0.9 * (group['papers'] == 1)) #Less If This Is Only Paper Made By Author (Dilutes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Generating New Embeddings\n",
    "\n",
    "- Can then apply these methods to generate alternative embeddings, allowing user to specify their desired base embedding to use (important if we are comparing methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "A Method That Generates An Alternative Embedding For A Paper, By Generating An Embedding For Each Author, Then Aggregating Each Papers Contributors Embeddings\n",
    "\"\"\"\n",
    "def generate_alternative_embeddings(chosen_embeddings, contributions):\n",
    "\n",
    "    #Generate Author Embedding\n",
    "    author_embeddings = (\n",
    "        contributions\n",
    "        .groupby('contributor')\n",
    "        .apply(lambda group: paper_contribution_to_author(group,chosen_embeddings), include_groups=False)\n",
    "        .reset_index(name = \"author_\" + chosen_embeddings)\n",
    "    )\n",
    "\n",
    "    #Merge To Join This Data\n",
    "    contributions = contributions.merge(author_embeddings, on='contributor', how='left')\n",
    "\n",
    "    #Generate New Paper Embedding\n",
    "    alternative_embeddings = (\n",
    "        contributions\n",
    "        .groupby('id')\n",
    "        .apply(lambda group: authors_contribution_to_paper(group, \"author_\" + chosen_embeddings), include_groups=False)\n",
    "        .sort_index()\n",
    "        .reset_index(name= \"alternative_\" + chosen_embeddings)\n",
    "    )\n",
    "\n",
    "    #Returning New Embedding\n",
    "    return list(alternative_embeddings[\"alternative_\" + chosen_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Generating New Embeddings\n",
    "\n",
    "- In addition to the 2 initial embeddings you had, added the joined version I added above\n",
    "\n",
    "- ALTERNATIVE COMBOS CAN BE TRIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['alternative_title_embeddings'] =    generate_alternative_embeddings( chosen_embeddings=\"title_embeddings\",    contributions=df_contributions)\n",
    "df_cleaned['alternative_abstract_embeddings'] = generate_alternative_embeddings(chosen_embeddings=\"abstract_embeddings\", contributions=df_contributions)\n",
    "df_cleaned['alternative_joined_embeddings'] = generate_alternative_embeddings(chosen_embeddings=\"joined_embeddings\",    contributions=df_contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9DI_-Tw0uS9"
   },
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "This step calculates the cosine similarity between research papers based on their BERT-generated embeddings for titles and abstracts. Cosine similarity quantifies the degree of similarity between two vectors, making it a valuable metric for identifying papers with related content or topics.\n",
    "\n",
    "The importance of this step is as follows:\n",
    "\n",
    "- Identifying Similar Papers: Cosine similarity allows for the identification of research papers that are thematically or conceptually similar, based on the content of their titles and abstracts.\n",
    "- Efficient Comparison: Cosine similarity offers a computationally efficient and effective method for comparing text data, facilitating quick similarity assessments.\n",
    "- Application in Recommendation Systems: This metric is integral for developing recommendation models, as it helps suggest papers that share common themes or concepts, aiding in the discovery of relevant literature and minimizing redundancy in research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xjoSuWmDxN6"
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between all papers based on their title and abstract embeddings\n",
    "title_sim_matrix = cosine_similarity(df_cleaned['title_embeddings'].tolist())\n",
    "abstract_sim_matrix = cosine_similarity(df_cleaned['abstract_embeddings'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJSeTUYV2eNE"
   },
   "source": [
    "### Category Similarity Comparison\n",
    "This code computes and visualizes the similarity between research paper categories based on their title and abstract embeddings. The function CompareAll calculates the mean similarity between categories by comparing each paper in one category to its most similar papers in another category, using a precomputed similarity matrix.\n",
    "\n",
    "The function iterates through all category pairs and finds the most similar articles across categories. It then computes an average similarity score for each category pair and stores the results in a difference matrix. The diagonal values, representing self-comparisons, are set to zero initially but later replaced with NaN for better visualization.\n",
    "\n",
    "A heatmap is then generated to display these similarity differences, using a color gradient to indicate how closely related different research categories are. This visualization helps identify relationships between research areas, revealing clusters of related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXnrGFTx2eNE",
    "outputId": "fafa6703-c4e4-4425-f11d-b76d2e6befea"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to compute cosine similarity matrix between categories using combined embeddings\n",
    "def compute_combined_category_similarity_matrix(title_column, abstract_column, df):\n",
    "\n",
    "    # Combine title and abstract embeddings by averaging them\n",
    "    combined_embeddings = np.array([0.5 * (title + abstract) for title, abstract in zip(df[title_column], df[abstract_column])])\n",
    "\n",
    "    # Compute cosine similarity between all combined embeddings\n",
    "    similarity_matrix = cosine_similarity(combined_embeddings)\n",
    "\n",
    "    # Get the unique categories\n",
    "    categories = df['subject_map'].unique()\n",
    "\n",
    "    # Initialize a matrix to store average similarity scores between categories\n",
    "    category_sim_matrix = np.zeros((len(categories), len(categories)))\n",
    "\n",
    "    # Loop over all pairs of categories\n",
    "    for i, cat1 in enumerate(categories):\n",
    "        for j, cat2 in enumerate(categories):\n",
    "\n",
    "            # Get the indices for both categories\n",
    "            indices1 = df[df['subject_map'] == cat1].index\n",
    "            indices2 = df[df['subject_map'] == cat2].index\n",
    "\n",
    "            # Get the cosine similarity between all pairs of documents in category cat1 and cat2\n",
    "            similarity_values = similarity_matrix[np.ix_(indices1, indices2)]\n",
    "\n",
    "            # Calculate the mean similarity for this category pair\n",
    "            category_sim_matrix[i, j] = np.mean(similarity_values)\n",
    "\n",
    "    # Convert the similarity matrix to a DataFrame with category labels\n",
    "    category_sim_df = pd.DataFrame(category_sim_matrix, index=categories, columns=categories)\n",
    "\n",
    "    return category_sim_df\n",
    "\n",
    "# Compute the similarity matrix using combined title and abstract embeddings\n",
    "category_similarity_df = compute_combined_category_similarity_matrix('title_embeddings', 'abstract_embeddings', df_cleaned)\n",
    "\n",
    "# Set diagonal to NaN to avoid self-similarity being shown\n",
    "np.fill_diagonal(category_similarity_df.values, np.nan)\n",
    "\n",
    "# Plot the heatmap for the category similarity matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    category_similarity_df,\n",
    "    cmap='YlGnBu',\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=0.5,\n",
    "    vmin=0.6,  # Set the minimum value for color scale\n",
    "    vmax=1,  # Set the maximum value for color scale\n",
    "    cbar_kws={'label': 'Cosine Similarity'}\n",
    ")\n",
    "\n",
    "plt.title(\"Cosine Similarity Matrix Between Categories (Combined Title & Abstract Embeddings)\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Category Similarity Comparison\n",
    "\n",
    "- An Alternative Method That Allows Inputted Embeddings To Be used, with level of specificity (IE: subject_map vs categories)\n",
    "- Method also makes use of above function to weigh each paper by above rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates Group Based Embedding Average\n",
    "\"\"\"\n",
    "def aggregate_groups(df, embedding, categories):\n",
    "    return (\n",
    "        df\n",
    "        .groupby(categories)\n",
    "        .apply(lambda group: paper_contribution_to_category(group, embedding), include_groups=False)\n",
    "        .reset_index(name='embeddings')\n",
    "        .sort_values(by=categories)\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generates A Similarity Matrix\n",
    "\"\"\"\n",
    "def plot_similarity_matrix(ax, similarity, columns, title):\n",
    "    visual = pd.DataFrame(similarity, index=columns, columns=columns)\n",
    "    sns.heatmap(visual, ax=ax, annot=True)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\"\"\"\n",
    "Generates An (Optionally Normalized) Matrix To Display Differences In Similarity Between Categories Between Methods\n",
    "\"\"\"\n",
    "def comparing_embedding_similarity(df, embedding1, embedding2, categories, normalize=True):\n",
    "\n",
    "    #Plotting Stuff\n",
    "    fig, axes = plt.subplots(1,3, figsize=(30, 10))\n",
    "\n",
    "    #Generating Grouped Embeddings\n",
    "    grouped_embedding1 = aggregate_groups(df, embedding1, categories)\n",
    "    grouped_embedding2 = aggregate_groups(df, embedding2, categories)\n",
    "\n",
    "    cs1 = cosine_similarity(np.stack(grouped_embedding1['embeddings'].values))\n",
    "    cs2 = cosine_similarity(np.stack(grouped_embedding2['embeddings'].values))\n",
    "\n",
    "    #Groups\n",
    "    columns =  grouped_embedding1[categories]\n",
    "\n",
    "    plot_similarity_matrix(axes[0], cs1, columns, embedding1)\n",
    "    plot_similarity_matrix(axes[1], cs2, columns, embedding2)\n",
    "\n",
    "\n",
    "    #Difference In Similarity\n",
    "    diff = cs1 - cs2\n",
    "\n",
    "    #Optionally Normalize\n",
    "    if normalize:\n",
    "        diff = (diff - diff.mean()) / diff.std()\n",
    "\n",
    "    plot_similarity_matrix(axes[2], diff, columns, embedding1)\n",
    "\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Plotting Similarity Matrixs\n",
    "\n",
    "- Comparing Methods (first sim, second sim, third normalized sim)\n",
    "\n",
    "- ALTERNATIVE COMBOS CAN BE TRIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_embedding_similarity(df_cleaned, 'alternative_joined_embeddings', \"joined_embeddings\", \"subject_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nAolFLC2eNE"
   },
   "source": [
    "### Paper Uniqueness Evaluation\n",
    "This code computes and visualizes the uniqueness of research papers in different categories using similarity matrices based on their title and abstract embeddings. The function Uniqueness calculates how distinct a paper is by measuring its similarity to papers in different categories.\n",
    "\n",
    "The function iterates through each paper and identifies the most similar papers from other categories. It then computes an average similarity score for these cross-category papers, which is used as a uniqueness score (higher similarity to other categories means lower uniqueness). These scores are stored in a new column, 'Uniquity', in df_cleaned.\n",
    "\n",
    "The code then groups the uniqueness scores by subject and calculates the mean uniqueness score for each category. It generates bar charts to visualize the results, where lower scores indicate more unique subjects, and higher scores indicate greater overlap with other categories.\n",
    "\n",
    "Importance:\n",
    "- Assessing Paper Uniqueness: This metric provides insight into how distinct research papers are in terms of content, helping to identify which areas of research are more or less unique.\n",
    "- Category Insights: The resulting visualizations show which research categories are more likely to overlap with others, which can guide further exploration of interdisciplinary research or highlight saturated topics.\n",
    "- Enhanced Decision-Making: Understanding uniqueness can help researchers identify niches in the literature, potentially guiding future research into less-explored areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM4bvydL9o8U"
   },
   "outputs": [],
   "source": [
    "def Uniqueness(similarity_matrix):\n",
    "    uniquity_scores = []  # Store uniqueness scores for each index\n",
    "\n",
    "    for j in range(len(similarity_matrix)):  # Iterate over all indices\n",
    "        mapped = df_cleaned.iloc[j]['subject_map']\n",
    "        similar_indices = [i for i in similarity_matrix[j].argsort()[::-1][1:]if df_cleaned.iloc[i]['subject_map'] != mapped]\n",
    "        sumb= 0\n",
    "        for i in similar_indices:\n",
    "            suma = similarity_matrix[j][i]\n",
    "            sumb += suma\n",
    "        lena = len(similar_indices)\n",
    "        uniquity = sumb / lena  # Compute uniqueness score\n",
    "\n",
    "        uniquity_scores.append(uniquity)  # Store score\n",
    "    df_cleaned['Uniquity'] = uniquity_scores  # Assign all scores to dataframe column\n",
    "    return uniquity_scores\n",
    "\n",
    "Uniqueness(title_sim_matrix)\n",
    "\n",
    "category_colors = {category: color for category, color in zip(categories, palette)}\n",
    "subject_colors = {subject: color for subject, color in zip(subjects, palette)}\n",
    "\n",
    "\n",
    "# Group by 'Subject' and calculate mean uniqueness\n",
    "subject_uniquity = df_cleaned.groupby('subject_map')['Uniquity'].mean().sort_values()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=subject_uniquity.index, y=subject_uniquity.values, palette=subject_colors)\n",
    "plt.xlabel('Average Uniquity Score (Lower is more Unique)')\n",
    "plt.ylabel('Subject')\n",
    "plt.title('Uniquity Score by Subject and Title')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "Uniqueness(abstract_sim_matrix)\n",
    "\n",
    "# Group by 'Subject' and calculate mean uniqueness\n",
    "subject_uniquity = df_cleaned.groupby('subject_map')['Uniquity'].mean().sort_values()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=subject_uniquity.index, y=subject_uniquity.values, palette=subject_colors)\n",
    "plt.xlabel('Average Uniquity Score (Lower is more Unique)')\n",
    "plt.ylabel('Subject')\n",
    "plt.title('Uniquity Score by Subject and Abstract')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIl8jInx2eNF"
   },
   "source": [
    "### PCA Visualization of Research Paper Distribution\n",
    "\n",
    "This code computes and visualizes the distribution of research papers in a lower-dimensional space using Principal Component Analysis (PCA) on their title embeddings. The PCA transformation reduces the high-dimensional embeddings into two principal components, allowing for a 2D visualization of the paper embeddings.\n",
    "\n",
    "The embeddings are first standardized using StandardScaler to ensure all features have a mean of 0 and a standard deviation of 1. PCA is then applied to project the data into a 2D space while preserving as much variance as possible.\n",
    "\n",
    "The code generates two scatter plots to visualize the PCA-transformed embeddings. These visualizations help identify clusters, outliers, and relationships between research categories and subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtj9xhS59HgF"
   },
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction (2D)\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(np.array(df_cleaned['title_embeddings'].tolist()))\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_embeddings)\n",
    "\n",
    "# Map categories to colors\n",
    "category_colors = {category: color for category, color in zip(categories, palette)}\n",
    "subject_colors = {subject: color for subject, color in zip(subjects, palette)}\n",
    "\n",
    "# Plot for mapped_categories\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x=pca_result[:, 0],\n",
    "    y=pca_result[:, 1],\n",
    "    hue=df_cleaned['mapped_categories'],\n",
    "    palette=category_colors,\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('PCA of Paper Embeddings Colored by Category')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Categories')\n",
    "plt.show()\n",
    "\n",
    "# Plot for subject_map\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x=pca_result[:, 0],\n",
    "    y=pca_result[:, 1],\n",
    "    hue=df_cleaned['subject_map'],\n",
    "    palette=subject_colors,\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('PCA of Paper Embeddings Colored by Subject')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Subjects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZjpEC-wv3z9"
   },
   "source": [
    "### Cosine Similarity and Sankey Diagram Visualization\n",
    "\n",
    "This code calculates the cosine similarity between the embeddings of research papers (based on their titles) and then generates a Sankey diagram to visualize the relationships between different research categories. By selecting a subset of categories from the subject_map column, it examines how papers within and across these categories compare in terms of content similarity.\n",
    "\n",
    "A Sankey diagram is a flow diagram that visually represents the flow of quantities between different categories or groups. The width of the arrows (or \"links\") is proportional to the quantity being represented. In this case, the arrows indicate the degree of similarity between research categories, with thicker links showing stronger relationships.\n",
    "\n",
    "Importance: \n",
    "- Understanding Relationships: It provides an intuitive way to understand the relationships between different research categories based on the content of the papers.\n",
    "- Revealing Interdisciplinary Connections: By visualizing category similarities, researchers can identify overlapping areas between disciplines, guiding potential areas for interdisciplinary research.\n",
    "- Efficient Exploration: The Sankey diagram offers a clear and compact way to explore complex similarity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo31jgwdrOzz"
   },
   "outputs": [],
   "source": [
    "# Select a subset of 4 categories from the 'subject_map' column\n",
    "desired_categories = df_cleaned['subject_map'].unique()[:4]\n",
    "\n",
    "# Filter `df_cleaned` to include only papers with the categories in desired_categories\n",
    "subset_df = df_cleaned[df_cleaned['subject_map'].isin(desired_categories)]\n",
    "\n",
    "# Ensure that we have only the desired categories (excluding \"Unknown Category\")\n",
    "subset_df = subset_df[subset_df['subject_map'] != 'Unknown Category']\n",
    "\n",
    "# Check if the subset_df has enough data\n",
    "print(f\"Number of papers in subset_df: {len(subset_df)}\")\n",
    "\n",
    "# Compute similarity matrix for selected papers\n",
    "paper_embeddings = np.stack(subset_df['title_embeddings'].values)\n",
    "similarity_matrix = cosine_similarity(paper_embeddings)\n",
    "\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "print(f\"Number of rows in subset_df: {len(subset_df)}\")\n",
    "\n",
    "# Compute category-to-category similarity (mean cosine similarity of papers)\n",
    "category_pairs = defaultdict(float)\n",
    "category_counts = defaultdict(int)\n",
    "\n",
    "# Compute category similarity for pairs\n",
    "for i, cat1 in enumerate(subset_df['subject_map']):\n",
    "    for j, cat2 in enumerate(subset_df['subject_map']):\n",
    "        if i < j:\n",
    "            category_pairs[(cat1, cat2)] += similarity_matrix[i, j]\n",
    "            category_counts[(cat1, cat2)] += 1\n",
    "\n",
    "# Compute average similarities\n",
    "category_avg_sim = {k: v / category_counts[k] for k, v in category_pairs.items()}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source, target, value = [], [], []\n",
    "for (cat1, cat2), sim in category_avg_sim.items():\n",
    "    if sim > 0.3:\n",
    "        source.append(cat1)\n",
    "        target.append(cat2)\n",
    "        value.append(sim)\n",
    "\n",
    "# Create Sankey diagram\n",
    "fig = go.Figure(go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=list(set(source + target))  # Unique category names\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=[list(set(source + target)).index(s) for s in source],\n",
    "        target=[list(set(source + target)).index(t) for t in target],\n",
    "        value=value\n",
    "    )\n",
    "))\n",
    "\n",
    "# Update layout for better spacing\n",
    "fig.update_layout(\n",
    "    title_text=\"Cross-Category Paper Similarity Sankey Diagram\",\n",
    "    font_size=10,\n",
    "    height=800,\n",
    "    width=1200\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KOjq2aywKLn"
   },
   "source": [
    "### Topic Modeling with LDA and Word Cloud Visualization\n",
    "\n",
    "This code uses Latent Dirichlet Allocation (LDA) to perform topic modeling on the cleaned text data (title + abstract) of research papers. LDA is a statistical model that uncovers hidden topics within large collections of text. It works by analyzing the distribution of words across documents and grouping them into topics. The code generates a document-term matrix and applies LDA to identify the topics, displaying the top words for each topic. Word clouds are then created to visually represent the most frequent terms associated with each topic.\n",
    "\n",
    "A word cloud is a visualization that shows words in varying sizes based on their frequency or significance in a given dataset. The most important or frequent words appear larger, making it easy to identify key themes or topics. In this case, the word clouds visually depict the key terms for each topic, helping to quickly capture the essence of each theme.\n",
    "\n",
    "Importance:\n",
    "*    Topic Discovery: LDA helps uncover the underlying themes or topics in large datasets, making it easier to understand the primary areas of focus in the research papers.\n",
    "\n",
    "*    Text Summarization: The word clouds offer a concise and visual representation of the most important words within each topic, allowing for quick exploration and understanding of the dataset.\n",
    "\n",
    "*    Improving Data Interpretation: Topic modeling aids in identifying common patterns or emerging trends in research, supporting better decision-making and guiding future research directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8JFZkj-sUJw"
   },
   "outputs": [],
   "source": [
    "# Apply CountVectorizer to the cleaned text data ('title' + 'abstract')\n",
    "text_data = df_cleaned['title'] + \" \" + df_cleaned['abstract']\n",
    "\n",
    "# Create a document-term matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "dtm = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Perform LDA (Latent Dirichlet Allocation)\n",
    "num_topics = 5  # Set the number of topics\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 5  # Number of top words to display for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# Word Cloud for Topics\n",
    "def plot_word_cloud(topic_idx):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        max_words=100,\n",
    "        colormap='viridis'\n",
    "    ).generate_from_frequencies(dict(zip(feature_names, lda_model.components_[topic_idx])))\n",
    "    plt.title(f\"Topic {topic_idx + 1} Word Cloud\")\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Plot word clouds for the topics\n",
    "for topic_idx in range(num_topics):\n",
    "    plot_word_cloud(topic_idx)\n",
    "\n",
    "# Assign dominant topic to each document\n",
    "lda_topic_distribution = lda_model.transform(dtm)\n",
    "df_cleaned['dominant_topic'] = lda_topic_distribution.argmax(axis=1)\n",
    "\n",
    "# Use raw topic numbers (convert them to strings for plotting consistency)\n",
    "df_cleaned['dominant_topic'] = df_cleaned['dominant_topic'].astype(str)\n",
    "\n",
    "# Count occurrences of each topic per category\n",
    "topic_category_counts = df_cleaned.groupby(['subject_map', 'dominant_topic']).size().reset_index(name='count')\n",
    "\n",
    "# Create the grouped bar chart (Topic Distribution Across Research Categories)\n",
    "fig = px.bar(\n",
    "    topic_category_counts,\n",
    "    x=\"subject_map\",\n",
    "    y=\"count\",\n",
    "    color=\"dominant_topic\",\n",
    "    barmode=\"group\",\n",
    "    title=\"LDA Topic Distribution Across Research Categories\",\n",
    "    labels={\"count\": \"Number of Papers\", \"subject_map\": \"Research Categories\", \"dominant_topic\": \"Topic\"},\n",
    "    color_discrete_sequence=px.colors.qualitative.Set3\n",
    ")\n",
    "\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()\n",
    "\n",
    "# Assign dominant topic to each document using LDA topic distribution\n",
    "lda_topic_distribution = lda_model.transform(dtm)\n",
    "df_cleaned['dominant_topic'] = lda_topic_distribution.argmax(axis=1)\n",
    "df_cleaned['dominant_topic'] = df_cleaned['dominant_topic']+1\n",
    "\n",
    "# Convert dominant topics to string for consistent labeling\n",
    "df_cleaned['dominant_topic'] = df_cleaned['dominant_topic'].astype(str) \n",
    "\n",
    "# Count occurrences of each topic within each research category\n",
    "category_per_topic = df_cleaned.groupby(['dominant_topic', 'subject_map']).size().reset_index(name='count')\n",
    "\n",
    "# Create a stacked bar chart (Dominant Categories in Each Topic)\n",
    "fig2 = px.bar(\n",
    "    category_per_topic,\n",
    "    x=\"dominant_topic\",\n",
    "    y=\"count\",\n",
    "    color=\"subject_map\",\n",
    "    barmode=\"stack\",\n",
    "    title=\"Dominant Categories in Each Topic (LDA)\",\n",
    "    labels={\"count\": \"Number of Papers\", \"dominant_topic\": \"Topic\", \"subject_map\": \"Research Categories\"},\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZhZXcXPwRzB"
   },
   "source": [
    "### Topic Modeling with NMF and Visualization\n",
    "\n",
    "This code uses Non-Negative Matrix Factorization (NMF) to identify topics within research papers by analyzing the combined title and abstract embeddings. NMF is a dimensionality reduction technique that decomposes the input data into two non-negative matrices, revealing hidden structures. The embeddings of the papers are first normalized, and then NMF is applied to discover latent topics. The dominant topic for each paper is assigned based on the highest value in its topic distribution. Two visualizations are generated:\n",
    "\n",
    "*   Topic Distribution Across Categories: A grouped bar chart showing how topics are distributed across different research categories.\n",
    "\n",
    "*   Dominant Categories in Each Topic: A stacked bar chart displaying the number of papers from each category within each identified topic.\n",
    "\n",
    "These visualizations help provide insights into how topics are represented in different categories and which categories dominate each topic.\n",
    "\n",
    "Importance:\n",
    "*   Topic Modeling: NMF allows for effective topic discovery by reducing the dimensionality of the embedding data, making it easier to group research papers based on their underlying topics.\n",
    "\n",
    "*   Data Exploration: The visualizations help in understanding how topics are distributed across research categories and identifying which categories are dominant within each topic, offering insights into research trends.\n",
    "\n",
    "*   Improved Decision-Making: By analyzing topic distributions, researchers can focus on emerging trends, refine categorization strategies, and guide future research directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jzCjyxasfbw"
   },
   "outputs": [],
   "source": [
    "df_new = df_cleaned.copy()\n",
    "\n",
    "# Combine Title and Abstract Embeddings\n",
    "df_new['combined_embeddings'] = df_new.apply(lambda row: np.concatenate([row['title_embeddings'], row['abstract_embeddings']]), axis=1)\n",
    "\n",
    "# Stack all embeddings to create a matrix of shape (num_documents, embedding_size)\n",
    "embeddings_matrix = np.vstack(df_new['combined_embeddings'].values)\n",
    "\n",
    "# Normalize the embeddings to make sure they are non-negative\n",
    "scaler = MinMaxScaler()\n",
    "embeddings_matrix_normalized = scaler.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Fit the NMF model on the normalized embeddings matrix\n",
    "nmf_model = NMF(n_components=6, random_state=42)  # Change n_components as needed\n",
    "nmf_model.fit(embeddings_matrix_normalized)\n",
    "\n",
    "# Assign dominant topic to each paper\n",
    "topic_distribution = nmf_model.transform(embeddings_matrix_normalized)\n",
    "df_new['dominant_topic'] = topic_distribution.argmax(axis=1)\n",
    "\n",
    "# Use raw topic numbers (no conversion to human-readable labels)\n",
    "df_new['dominant_topic'] = df_new['dominant_topic'].astype(str)  # Convert to string for plotting\n",
    "\n",
    "# Count occurrences of each topic per category\n",
    "topic_category_counts = df_new.groupby(['subject_map', 'dominant_topic']).size().reset_index(name='count')\n",
    "\n",
    "# Create the first grouped bar chart (Topic Distribution Across Research Categories)\n",
    "fig1 = px.bar(\n",
    "    topic_category_counts,\n",
    "    x=\"subject_map\",\n",
    "    y=\"count\",\n",
    "    color=\"dominant_topic\",\n",
    "    barmode=\"group\",\n",
    "    title=\"Topic Distribution Across Research Categories (Raw Topics)\",\n",
    "    labels={\"count\": \"Number of Papers\", \"subject_map\": \"Research Categories\", \"dominant_topic\": \"Topic\"},\n",
    "    color_discrete_sequence=px.colors.qualitative.Set3\n",
    "\n",
    ")\n",
    "\n",
    "fig1.update_layout(xaxis_tickangle=-45)\n",
    "fig1.show()\n",
    "category_per_topic = category_per_topic[category_per_topic['dominant_topic'] != '0']\n",
    "\n",
    "# Create a second plot showing the dominant categories for each topic\n",
    "category_per_topic = df_new.groupby(['dominant_topic', 'subject_map']).size().reset_index(name='count')\n",
    "\n",
    "# Remove the first topic (smallest value)\n",
    "min_topic = category_per_topic['dominant_topic'].astype(int).min()\n",
    "category_per_topic = category_per_topic[category_per_topic['dominant_topic'] != str(min_topic)]\n",
    "\n",
    "fig2 = px.bar(\n",
    "    category_per_topic,\n",
    "    x=\"dominant_topic\",\n",
    "    y=\"count\",\n",
    "    color=\"subject_map\",\n",
    "    barmode=\"stack\",\n",
    "    title=\"Dominant Categories in Each Topic\",\n",
    "    labels={\"count\": \"Number of Papers\", \"dominant_topic\": \"Topic\", \"subject_map\": \"Research Categories\"},\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22816G1xGzmL"
   },
   "source": [
    "###  GUI for Article Recommendations\n",
    "This code implements a simple graphical user interface (GUI) using the Tkinter library to allow users to find articles similar to a given article based on their title. The main function get_recommendations retrieves the top N articles similar to the user-provided article ID, and get_recommendationsOF fetches similar articles from different categories. The interface includes an input field for entering the article ID, a search button, and displays the title and category of the input article along with the top N recommendations.\n",
    "\n",
    "When the user enters a valid article ID, the recommendations are displayed in two sections: one showing similar articles and the other displaying articles from different categories. The application provides an intuitive and visually appealing way to explore research papers and their relationships based on content similarity and category.\n",
    "\n",
    "**Note: This GUI is designed to be run on a local desktop environment, such as through an IDE like Visual Studio Code (VSCode) or directly on your computer. It is not suitable to run in a cloud-based environment like Google Colab, as Colab does not support interactive desktop applications like Tkinter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3xWe-Da95xH"
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "\n",
    "def get_recommendations(index, top_n=5, similarity_matrix=title_sim_matrix):\n",
    "\n",
    "    similar_indices = similarity_matrix[index].argsort()[::-1][1:top_n + 1]\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    for idx in similar_indices:\n",
    "\n",
    "        recommendations.append({\n",
    "            'paper_title': df_cleaned.iloc[idx]['title'],\n",
    "            'similarity_score': similarity_matrix[index][idx]\n",
    "        })\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "def get_recommendationsOF(index, top_n=5, similarity_matrix=title_sim_matrix):\n",
    "\n",
    "    mapped = df_cleaned.iloc[index]['subject_map']\n",
    "\n",
    "    similar_indices = [i for i in similarity_matrix[index].argsort()[::-1][1:]if df_cleaned.iloc[i]['subject_map'] != mapped][:top_n]\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    for idx in similar_indices:\n",
    "\n",
    "        recommendations.append({\n",
    "            'paper_title': df_cleaned.iloc[idx]['title'],\n",
    "            'similarity_score': similarity_matrix[index][idx],\n",
    "            'mapped_categories1': df_cleaned.iloc[idx]['mapped_categories']\n",
    "        })\n",
    "\n",
    "    return recommendations, mapped\n",
    "\n",
    "# Function to handle button click\n",
    "def on_button_click():\n",
    "\n",
    "    user_input = entry.get().strip()\n",
    "\n",
    "    if not user_input:\n",
    "\n",
    "        result_label.config(text=\"Please enter a valid article ID.\", fg=\"red\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "\n",
    "        intValue = int(user_input)  # Ensure input is a number\n",
    "        recommendations = get_recommendations(intValue, top_n=5, similarity_matrix=title_sim_matrix)\n",
    "        recommendationsOF, mapped = get_recommendationsOF(intValue, top_n=5, similarity_matrix=title_sim_matrix)\n",
    "\n",
    "        # Formatting recommendations\n",
    "        result_text = \"\\n\".join(\n",
    "            f\" Recommendation {i + 1}: {rec['paper_title']} (Similarity: {rec['similarity_score']:.3f})\"\n",
    "            for i, rec in enumerate(recommendations)\n",
    "        )\n",
    "\n",
    "        result_label.config(text=result_text, fg=\"black\")\n",
    "\n",
    "        result_text2 = \"\\n\".join(\n",
    "            f\" Recommendation {i + 1}: {rec['paper_title']} (Similarity: {rec['similarity_score']:.3f}) (Category: {rec['mapped_categories1']})\"\n",
    "            for i, rec in enumerate(recommendationsOF)\n",
    "        )\n",
    "\n",
    "        result_label2.config(text=result_text2, fg=\"black\")\n",
    "\n",
    "        # Update labels\n",
    "        label.config(text=f\" Title of Article: {df_cleaned.iloc[intValue]['title']}\")\n",
    "        cat.config(text=f\" Category: {mapped}\")\n",
    "\n",
    "    except (ValueError, IndexError):\n",
    "\n",
    "        result_label.config(text=\"Invalid article ID.\", fg=\"red\")\n",
    "\n",
    "# Create main window\n",
    "root = tk.Tk()\n",
    "root.title(\" Similar Article Finder\")\n",
    "root.geometry(\"1200x500\")\n",
    "root.configure(bg=\"#F4F4F4\")  # Light gray background\n",
    "\n",
    "# Styling\n",
    "HEADER_FONT = (\"Segoe UI Bold\", 14)\n",
    "LABEL_FONT = (\"Segoe UI\", 11)\n",
    "RESULT_FONT = (\"Segoe UI\", 10)\n",
    "\n",
    "# Title Label\n",
    "title_label = tk.Label(root, text=\" Find Similar Articles\", font=HEADER_FONT, bg=\"#F4F4F4\", fg=\"#2C3E50\")\n",
    "title_label.pack(pady=10)\n",
    "\n",
    "# Input Section\n",
    "input_frame = tk.Frame(root, bg=\"#F4F4F4\")\n",
    "input_frame.pack(pady=5)\n",
    "\n",
    "label = tk.Label(input_frame, text=\"Enter an Article ID:\", font=LABEL_FONT, bg=\"#F4F4F4\")\n",
    "label.grid(row=0, column=0, padx=5)\n",
    "\n",
    "entry = tk.Entry(input_frame, font=(\"Arial\", 12), width=10)\n",
    "entry.grid(row=0, column=1, padx=5)\n",
    "\n",
    "button = tk.Button(input_frame, text=\"Search\", command=on_button_click, font=LABEL_FONT, bg=\"#3498DB\", fg=\"white\", padx=10)\n",
    "button.grid(row=0, column=2, padx=5)\n",
    "\n",
    "cat = tk.Label(root, text=\"\", font=LABEL_FONT, bg=\"#F4F4F4\", fg=\"#2C3E50\")\n",
    "cat.pack(pady=5)\n",
    "\n",
    "# Similar Articles Section\n",
    "frame1 = tk.Frame(root, bg=\"white\", bd=2, relief=\"groove\", padx=10, pady=5)\n",
    "frame1.pack(fill=\"both\", padx=20, pady=10)\n",
    "\n",
    "title1 = tk.Label(frame1, text=\" Similar Articles\", font=HEADER_FONT, fg=\"#27AE60\", bg=\"white\")\n",
    "title1.pack(anchor=\"w\")\n",
    "\n",
    "result_label = tk.Label(frame1, text=\"\", font=RESULT_FONT, bg=\"white\", justify=\"left\", anchor=\"w\")\n",
    "result_label.pack(pady=5, anchor=\"w\")\n",
    "\n",
    "# Other Category Articles Section\n",
    "frame2 = tk.Frame(root, bg=\"white\", bd=2, relief=\"groove\", padx=10, pady=5)\n",
    "frame2.pack(fill=\"both\", padx=20, pady=10)\n",
    "\n",
    "title2 = tk.Label(frame2, text=\" Articles from Other Categories\", font=HEADER_FONT, fg=\"#E67E22\", bg=\"white\")\n",
    "title2.pack(anchor=\"w\")\n",
    "\n",
    "result_label2 = tk.Label(frame2, text=\"\", font=RESULT_FONT, bg=\"white\", justify=\"left\", anchor=\"w\")\n",
    "result_label2.pack(pady=5, anchor=\"w\")\n",
    "\n",
    "# Run the application\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
